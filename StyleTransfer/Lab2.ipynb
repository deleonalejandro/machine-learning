{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "retired-cornell",
   "metadata": {},
   "source": [
    "# Lab 2: Style Transfer\n",
    "*Team Members:*\n",
    "- Yasmin Femerling\n",
    "- Alejandro de Leon\n",
    "\n",
    "March 21, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-fault",
   "metadata": {},
   "source": [
    "In this lab we implement a photo realistic style transfer algorithm using the work of Li et al. in their universal style transfer paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprised-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, applications, losses\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import get_file\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib \n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.python.framework.ops import enable_eager_execution\n",
    "enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-syndicate",
   "metadata": {
    "tags": []
   },
   "source": [
    "- vgg tiene stride de 1\n",
    "- convolutional layers de 3x3\n",
    "- cambiamos el numero de las layers\n",
    "- cambiamos el size del conv2d del upsampling pq tiene que ser un factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version from internet not sense at all\n",
    "def decoder_layers(inputs, layer):\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "    if layer == 1:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "    x = Conv2D(512, (4, 4), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    if layer == 2:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "    x = Conv2D(256, (4, 4), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "    if layer == 3:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "    x = Conv2D(128, (4, 4), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    if layer == 4:\n",
    "        return xdd\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "    x = Conv2D(64, (4, 4), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    if layer == 5:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_num_samples(directory):\n",
    "    total=0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total\n",
    "\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        WEIGHTS_PATH_NO_TOP,\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    f = h5py.File(WEIGHTS_PATH)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            g = f[b_name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL\n",
    "\n",
    "def VGG19(img_in=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    print(\"entre a vgg\")\n",
    "    if img_in is not None:\n",
    "        print(\"si tengo imagen\")\n",
    "        if len(K.int_shape(img_in)) == 3:\n",
    "            img_in = K.expand_dims(img_in, axis=0)\n",
    "            print(\"expandi dims\", img_in.shape)\n",
    "        \n",
    "        img_in = preprocess_input(img_in)\n",
    "        print(\"ya me preprocesaron\", img_in.shape)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if img_in is None:\n",
    "        print(\"no tenia imagen\")\n",
    "        inputs = Input(shape=input_shape)\n",
    "        print(\"Se creo el input con tensor\")\n",
    "    else:\n",
    "        inputs = Input(tensor=img_in, shape=input_shape)\n",
    "        print(\"Se creo el input con tensor\")\n",
    "        \n",
    "    print(inputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name=\"vgg19\")\n",
    "    load_weights(model)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-dutch",
   "metadata": {},
   "source": [
    "- training decoder from early layer ('block2_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occupational-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA=1\n",
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "    \n",
    "## STYLE-TRANSFER LOSS\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        \n",
    "        print(\"Creando modelo...\")\n",
    "        self.encoder = VGG19(input_shape=self.input_shape, target_layer=target_layer)\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "        \n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "        \n",
    "        self.loss = self.create_loss_fn()\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "        self.model.summary()\n",
    "        print(\"Termine modelo...\")\n",
    "        \n",
    "    def create_loss_fn(self):\n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            content_image = y_true\n",
    "            reconstructed_image = y_pred\n",
    "            \n",
    "            encoding_in = self.encoder(content_image)\n",
    "            encoding_out = self.encoder(reconstructed_image)\n",
    "            \n",
    "            print(\"recon\", reconstructed_image.shape)\n",
    "            print(\"content\", content_image.shape)\n",
    "            print(\"en_out\", encoding_out.shape)\n",
    "            print(\"en_in\", encoding_in.shape)\n",
    "            return l2_loss(reconstructed_image - content_image) + \\\n",
    "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        \n",
    "        return custom_loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same', name='decoder_out')(layers)\n",
    "        decoder = Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "        print(decoder.summary())\n",
    "        return decoder\n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_night_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-track",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Creando modelo...\n",
      "entre a vgg\n",
      "no tenia imagen\n",
      "Se creo el input con tensor\n",
      "Tensor(\"input_1:0\", shape=(None, 256, 256, 3), dtype=float32)\n",
      "Model: \"decoder_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 128, 128)]   0         \n",
      "_________________________________________________________________\n",
      "decoder_block5_conv1 (Conv2D (None, 128, 128, 512)     590336    \n",
      "_________________________________________________________________\n",
      "decoder_block4_upsample (UpS (None, 256, 256, 512)     0         \n",
      "_________________________________________________________________\n",
      "decoder_block4_conv4 (Conv2D (None, 256, 256, 512)     4194816   \n",
      "_________________________________________________________________\n",
      "decoder_block4_conv3 (Conv2D (None, 256, 256, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "decoder_block4_conv2 (Conv2D (None, 256, 256, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "decoder_block4_conv1 (Conv2D (None, 256, 256, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "decoder_out (Conv2D)         (None, 256, 256, 3)       13827     \n",
      "=================================================================\n",
      "Total params: 11,878,403\n",
      "Trainable params: 11,878,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-bde0ca733624>:55: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(WEIGHTS_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon (None, 256, 256, 3)\n",
      "content (None, 256, 256, 3)\n",
      "en_out (None, 128, 128, 128)\n",
      "en_in (None, 128, 128, 128)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 128, 128, 128)     112576    \n",
      "_________________________________________________________________\n",
      "decoder_2 (Functional)       (None, 256, 256, 3)       11878403  \n",
      "=================================================================\n",
      "Total params: 11,990,979\n",
      "Trainable params: 11,878,403\n",
      "Non-trainable params: 112,576\n",
      "_________________________________________________________________\n",
      "Termine modelo...\n",
      "Epoch 1/4\n",
      "125/125 [==============================] - ETA: 0s - batch: 62.0000 - size: 8.0000 - loss: 140573331521667.0781 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro de Leon\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 7458s 60s/step - batch: 62.0000 - size: 8.0000 - loss: 140573331521667.0781 - val_loss: 286280386805.7600\n",
      "Epoch 2/4\n",
      "125/125 [==============================] - 7166s 57s/step - batch: 62.0000 - size: 8.0000 - loss: 282534414647.2960 - val_loss: 283699775078.4000\n",
      "Epoch 3/4\n",
      "125/125 [==============================] - 7312s 59s/step - batch: 62.0000 - size: 8.0000 - loss: 282534415958.0160 - val_loss: 297394058690.5600\n",
      "Epoch 4/4\n",
      "125/125 [==============================] - 7588s 61s/step - batch: 62.0000 - size: 8.0000 - loss: 282534418972.6720 - val_loss: 279942553600.0000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = 'dogcat/dataset/train-chill'\n",
    "VAL_PATH = 'dogcat/dataset/test-chill' \n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "epochs = 4\n",
    "target_layer = 2\n",
    "        \n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "\n",
    "# Creating generators\n",
    "train_gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "validation_gen = create_gen(VAL_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "\n",
    "# Steps per epoc calculation\n",
    "num_samples = count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "\n",
    "# Validation steps calculation\n",
    "num_samples = count_num_samples(VAL_PATH)\n",
    "validation_steps = num_samples // BATCH_SIZE\n",
    "\n",
    "# Initializing encoder-decoder\n",
    "#K.clear_session()\n",
    "encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
    "\n",
    "\n",
    "# Training model\n",
    "H = encoder_decoder.model.fit(train_gen, \n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              validation_data=validation_gen,\n",
    "                              validation_steps=validation_steps,\n",
    "                              epochs = epochs, \n",
    "                             )\n",
    "\n",
    "# Saving decoder\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-penny",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Night"
   ]
  },
  {
   "cell_type": "raw",
   "id": "understood-dominant",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "vgg19 (Functional)           (None, 32, 32, 512)       3505728   \n",
    "_________________________________________________________________\n",
    "decoder_4 (Functional)       (None, 256, 256, 3)       7069251   \n",
    "=================================================================\n",
    "Total params: 10,574,979\n",
    "Trainable params: 7,069,251\n",
    "Non-trainable params: 3,505,728\n",
    "_________________________________________________________________\n",
    "Termine modelo...\n",
    "Epoch 1/4\n",
    "125/125 [==============================] - ETA: 0s - batch: 62.0000 - size: 8.0000 - loss: 1485902582382.5920 \n",
    "C:\\Users\\Alejandro de Leon\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
    "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
    "125/125 [==============================] - 3008s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1485902582382.5920 - val_loss: 1436333167083.5200\n",
    "Epoch 2/4\n",
    "125/125 [==============================] - 2995s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1428112203055.1040 - val_loss: 1442234121584.6399\n",
    "Epoch 3/4\n",
    "125/125 [==============================] - 2998s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1428112221929.4719 - val_loss: 1419905225195.5200\n",
    "Epoch 4/4\n",
    "125/125 [==============================] - 2998s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1428112230842.3682 - val_loss: 1421710665973.7600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-union",
   "metadata": {},
   "source": [
    "## Encoder-Decoder A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "restricted-proposal",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "vgg19 (Functional)           (None, 128, 128, 128)     112576    \n",
    "_________________________________________________________________\n",
    "decoder_2 (Functional)       (None, 256, 256, 3)       11878403  \n",
    "=================================================================\n",
    "Total params: 11,990,979\n",
    "Trainable params: 11,878,403\n",
    "Non-trainable params: 112,576\n",
    "_________________________________________________________________\n",
    "Termine modelo...\n",
    "Epoch 1/2\n",
    "2000/2000 [==============================] - 46536s 23s/step - batch: 999.5000 - size: 4.0000 - loss: 669974751655.9360\n",
    "Epoch 2/2\n",
    "2000/2000 [==============================] - 52873s 26s/step - batch: 999.5000 - size: 4.0000 - loss: 126673469906.9440"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-twins",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluate decoder\n",
    "We now test and evaluate the encoder-decoder with an image reconstruction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "tutorial-wayne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando modelo...\n",
      "entre a vgg\n",
      "no tenia imagen\n",
      "Se creo el input sin tensor\n",
      "Tensor(\"input_76:0\", shape=(None, 256, 256, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-f63158ec8c47>:55: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(WEIGHTS_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "recon (None, 256, 256, 3)\n",
      "content (None, 256, 256, 3)\n",
      "en_out (None, 32, 32, 512)\n",
      "en_in (None, 32, 32, 512)\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 32, 32, 512)       3505728   \n",
      "_________________________________________________________________\n",
      "decoder_4 (Functional)       (None, 256, 256, 3)       7069251   \n",
      "=================================================================\n",
      "Total params: 10,574,979\n",
      "Trainable params: 7,069,251\n",
      "Non-trainable params: 3,505,728\n",
      "_________________________________________________________________\n",
      "Termine modelo...\n"
     ]
    }
   ],
   "source": [
    "DECODER_PATH = 'decoder_4.h5'\n",
    "INPUT_IMG_PATH = 'dogcat/dataset/train-chill/cats/cat.3.jpg'\n",
    "OUTPUT_IMG_PATH = 'dog_recon.jpg'\n",
    "TARGET_LAYER = 4\n",
    "\n",
    "encoder_decoder = EncoderDecoder(decoder_path=DECODER_PATH, target_layer=TARGET_LAYER)\n",
    "\n",
    "# We load the test image from the input path\n",
    "input_img = image.load_img(INPUT_IMG_PATH, target_size = (256, 256))\n",
    "\n",
    "# Transform the image into an array\n",
    "input_img = image.img_to_array(input_img)\n",
    "\n",
    "# Preprocess\n",
    "input_img = preprocess_input(input_img)\n",
    "\n",
    "# Expand dimensions to match the input of the model 'batches'\n",
    "input_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "# Use the model to predict\n",
    "output_img = encoder_decoder.model.predict([input_img])[0]\n",
    "\n",
    "output_img /= np.amax(output_img)\n",
    "\n",
    "# Save the image\n",
    "plt.imsave(OUTPUT_IMG_PATH, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-internet",
   "metadata": {},
   "source": [
    "## Encoder-Decoder B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-personal",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "exceptional-thunder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEaCAYAAADg2nttAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABAzElEQVR4nO3deVxUZf//8dcZhmFfHQQRFTW10FzSgNKIklxy6zbF/c7bJf1lmVpWWt/oLksLLe0OLU2zzLtssbrNsMQ0t8wMrQyXwH1BRVCQfZjz+0OdJJF95gzD5/l4+Ig527wvhs7nXNeZc46iqqqKEEIIUQ6d1gGEEELYPykWQgghKiTFQgghRIWkWAghhKiQFAshhBAVkmIhhBCiQlIshLjGpk2bUBSFEydOVGk9RVH48MMPrZRKCO0pcp2FqIsURSl3frNmzThy5EiVt1tUVERmZiYNGzZEp6v8sVR6ejq+vr64urpW+T2rSlEUVqxYwciRI63+XkJcpdc6gBDVcfr0acvP27dv58EHHyQ5OZlGjRoB4OTkVGr5oqIiDAZDhds1GAwEBQVVOU911hGiLpFhKFEnBQUFWf75+/sDEBAQYJnWsGFD3nzzTYYPH46Pjw+jRo0C4Nlnn+WWW27B3d2dJk2aMHHiRC5evGjZ7t+Hoa6+Xr9+PVFRUbi7uxMWFkZiYmKpPH8fhlIUhYULFzJq1Ci8vLwICQlh9uzZpdY5f/48gwcPxsPDg8DAQP7v//6Phx56iJiYmBr9bt5//33CwsIwGAyEhITw3HPPYTKZLPO3bt1K165d8fLywsvLiw4dOvDtt99a5r/yyiu0aNECFxcXAgIC6NmzJ/n5+TXKJOo+KRbCYf373//mzjvvJDk5mVmzZgHg5ubG4sWLSUlJYfny5WzatInJkydXuK0nn3ySmTNn8uuvvxIREcGQIUPIysqq8P2joqLYs2cPM2bMYObMmWzYsMEy/1//+he//vorX3/9Nd9//z0nTpzgyy+/rFGb165dy5gxYxg1ahR79+5l3rx5JCQk8O9//xsAk8lE//79iYiIIDk5meTkZF544QXc3d0BWL16NXPmzGHBggX8+eefrF+/nt69e9cok3AQqhB13MaNG1VAPX78uGUaoI4ZM6bCdVevXq0aDAa1pKSkzG1dff35559b1klPT1cBdd26daXeb8WKFaVeP/bYY6Xe6+abb1afeeYZVVVV9eDBgyqgJiUlWeYXFRWpISEhavfu3cvN/Pf3ula3bt3UwYMHl5o2f/581dXVVS0sLFQzMzNVQN24cWOZ67/++utqq1at1KKionIziPrHoXsWCxcuZNy4cTzxxBMVLpuSksLTTz/N0KFD2bFjh2X6kSNHePbZZ5k2bRpPPvkk27dvt2ZkUYvCw8Ovm7Z69WqioqIIDg7G09OTESNGUFRURHp6ernb6tixo+XnwMBAnJycOHPmTKXXAQgODrask5KSAkBkZKRlvrOzM126dCl3mxX5448/iIqKKjXt7rvvpqCggLS0NPz8/Bg3bhw9e/akd+/ezJkzhwMHDliWjY2Npbi4mGbNmjF69GhWrFhBTk5OjTIJx+DQxSI6OpqZM2dWalmj0cgjjzxCt27dSk03GAw8+uijvP7668ycOZPly5eTm5trjbiilnl4eJR6/dNPPzF48GCioqL44osvSE5O5u233wYunwAvT1knx81mc5XWURTlunUq+laXNSxZsoRffvmF++67jx9++IF27drxzjvvANC4cWP279/PsmXLaNiwIS+99BJt2rTh+PHjNs8p7ItDF4uwsDA8PT1LTUtPT+fll1/m6aef5vnnn+fkyZMANGzYkGbNml33P29wcLDlGzb+/v74+PiQnZ1tmwaIWrV161aMRiOzZs0iIiKC1q1bV/l6itoSFhYGwI8//miZZjKZ+OWXX2q03bZt27J58+ZS03744Qfc3Nxo2bKlZVq7du2YNm0aiYmJjB07lsWLF1vmubi40KtXL1577TV+//138vLyanwuRdR99e6rs4sXL2b8+PE0atSIP//8k3fffZe4uLhKrZuamorJZCIwMNDKKYU1tGnThnPnzrF06VLuuecetm7dysKFCzXJ0qpVK/r168ekSZN45513CAgIYN68eWRnZ1eqt3Hs2DH27NlTalpwcDAzZsygX79+zJkzh4EDB7Jnzx5eeOEFnnjiCQwGA6mpqSxZsoR+/frRpEkTTp06xZYtW7jtttsAWLp0KWazmfDwcHx9fdmwYQM5OTmW4ibqr3pVLAoKCjhw4ACvv/66Zdq1XyksT1ZWFv/5z3+YNGlSlS7WEvajb9++PPvss8ycOZNLly5x9913Ex8fz/DhwzXJ89577zFhwgR69+6Np6cnEydO5L777qOgoKDCdZ999lmeffbZUtNmz57NM888w7Jly5gzZw7PP/88AQEBPPLII5YDIg8PD/7880+GDh3KuXPnaNCgAX369GHu3LkA+Pn5MXfuXJ566ikKCwtp0aIFixcvpnv37rX/CxB1isNfwX327FleffVV5s2bR15eHlOmTCnV5f67hIQEOnfuXOrEY15eHv/+97/5xz/+UWq6ELWppKSEm2++mf79+zNv3jyt4whRSr06RHZ3d6dhw4aWcWJVVSu8JYTJZGLu3LlERUVJoRC1avPmzXz22WekpaWxZ88exowZw5EjRxg9erTW0YS4jkP3LObPn09KSgo5OTn4+PgQGxtLu3btWLJkCRcuXMBkMtG1a1cGDRpEamoqc+fOJTc3F2dnZ3x9fXn99dfZvHkzixYtIiQkxLLdSZMmERoaql3DhEPYuHEjU6dOJTU1FWdnZ9q1a8fs2bOv+0aeEPbAoYuFEEKI2lGvhqGEEEJUjxQLIYQQFXLYr86eOnWq2usajUYyMjJqMY02HKUdIG2xV47SFkdpB9SsLcHBwTecJz0LIYQQFZJiIYQQokJSLIQQQlRIioUQQogKSbEQQghRISkWQgghKiTFQgghRIWkWFxDLcjHvPoDTKe1eSCOEELYKykW18rPQ/3+ay4t/4/WSYQQwq5IsbiG4tcApU8shTu3oO5N1jqOEELYDSkWf6PEDMApqDHmVUtQTcVaxxFCCLsgxeJvFGdnvMZMgfSTqN+v1TqOEELYBSkWZTB0uRPadUZd8xHqxSyt4wghhOakWJRBURR0Q8ZCcTHqFx9oHUcIITQnxeIGlKAQlJh+qNs2oB4+qHUcIYTQlBSLcih9hoCPH+aPFqOazVrHEUIIzUixKIfi5o4y8CE4fBD1x41axxFCCM1IsaiAEhkNLdqgrn4fNS9X6zhCCKEJKRYVUHQ6dMMehpyLqGtXaR1HCCE0IcWiEpTQVijd7kPdsAZV7hslhKiHpFhUkvLASDC4Yv54Caqqah1HCCFsSopFJSnevij9h0HKbvh1p9ZxhBDCpqRYVIESfT80aoL5k6WoxUVaxxFCCJuxSbFYuHAh48aN44knnih3udTUVIYOHcqOHTss0z788EOmTZvG1KlTWbZsmaZDQIpej27oeDiXjvrdl5rlEEIIW7NJsYiOjmbmzJnlLmM2m1m5ciUdOnSwTDtw4AAHDhxg7ty5zJs3j7S0NFJSUqwdt1xKWEe47Q7Ubz5FzTynaRYhhLAVmxSLsLAwPD09y10mMTGRiIgIvL29LdMURaGoqAiTyURxcTElJSX4+PhYO26FdIPHgKqifrZc6yhCCGETeq0DAGRmZrJz507i4uJYtGiRZXrr1q1p27YtDz/8MKqq0qtXL0JCQsrcRlJSEklJSQDMmTMHo9FY7Tx6vb789Y1GLv1jJLmfLMN7wFAMbTtV+72sqcJ21CHSFvvkKG1xlHaA9dpiF8Vi+fLljBgxAp2udEcnPT2dkydP8vbbbwPw0ksvsW/fPm655ZbrthETE0NMTIzldUZGRrXzGI3GCtdXo3pD0v/Iejse3XNvoDg5Vfv9rKUy7agrpC32yVHa4ijtgJq1JTg4+Ibz7KJYpKWlsWDBAgCys7PZvXs3Op2O9PR0WrVqhaurKwCdOnXi4MGDZRYLW1NcXNANHoP5nddQt3yLEn2/1pGEEMJq7KJYJCQklPq5c+fOhIeHs337djZs2EBJSQmqqpKSksL999vRTrlzV2hzK+qXK1G7dEPx9K54HSGEqINsUizmz59PSkoKOTk5TJw4kdjYWEwmEwA9evS44XqRkZHs3buXJ598EoCOHTvSpUsXW0SuFEVR0A0dj/nFKahf/RdlxEStIwkhhFXYpFhMmTKl0stOmjTJ8rNOp+Phhx+2QqLao4SEokT3Rt2UiBrVE6VJc60jCSFErZMruGuBMmA4eHhg/nix3DdKCOGQpFjUAsXDC+WBUXDwD9RdW7WOI4QQtU6KRS1R7roPmjRH/fQ91MICreMIIUStkmJRSxSdE7phEyArAzXxM63jCCFErZJiUYuUVmEoEXejfvsF6rl0reMIIUStkWJRy5QHR4OTE+ZPlmkdRQghao0Ui1qm+DVA6RMLe3ag/rFb6zhCCFErpFhYgRIzAAKCLj+C9crFh0IIUZdJsbACxdkZ3ZDxkH4CdeNareMIIUSNSbGwlvZdoF1n1DUfoWZnaZ1GCCFqRIqFlSiKgm7IWCgqQl29Qus4QghRI1IsrEgJCkGJ6Ye6LQn18EGt4wghRLVJsbAypc8Q8PHD/NFiVLNZ6zhCCFEtUiysTHFzRxn4EBw+iLpjo9ZxhBCiWqRY2IASGQ0t2qB+/j5qXq7WcYQQosqkWNiAotOhG/ow5FxEXbtK6zhCCFFlUixsRGneCqVrDOqGNainT2gdRwghqkSKhQ0p/xgFBpfLV3bLQ5KEEHWIFAsbUrx9UfoPg5Td8OtOreMIIUSlSbGwMSW6DzRqgvmTpajFRVrHEUKISpFiYWOKXo9u6Dg4l4763ZdaxxFCiErR2+JNFi5cSHJyMj4+PsybN++Gy6WmpvLcc88xZcoUIiMj2bt3L++//75l/qlTp3j88ccJDw+3RWyrUcI6QadI1G8+Rb3jXhR/o9aRhBCiXDbpWURHRzNz5sxylzGbzaxcuZIOHTpYprVr1474+Hji4+OJi4vDYDCUml+X6QaPAVVF/Xy51lGEEKJCNikWYWFheHp6lrtMYmIiEREReHt7lzl/x44ddOrUCRcXF2tEtDklIAil50DUnZtRD/6hdRwhhCiXTYahKpKZmcnOnTuJi4tj0aJFZS6zbds2+vbte8NtJCUlkZSUBMCcOXMwGqs/tKPX62u0fmWpIx8mY8dGdJ8uw3/uMhQnp1rdvq3aYQvSFvvkKG1xlHaA9dpiF8Vi+fLljBgxAp2u7I5OVlYWx44dK3cIKiYmhpiYGMvrjIyMaucxGo01Wr9KHnwI0zuvce6L/6KL7l2rm7ZpO6xM2mKfHKUtjtIOqFlbgoODbzjPLopFWloaCxYsACA7O5vdu3ej0+ksJ7J//PFHwsPD0evtIm7t6twV2tyK+uWHqLd3Q/Hw0jqREEJcxy72vgkJCaV+7ty5c6lvPG3bto1hw4ZpEc3qFEVBN3Q85henoH61EmX4RK0jCSHEdWxSLObPn09KSgo5OTlMnDiR2NhYTCYTAD169Ch33bNnz5KRkUFYWJgtompCCQlFie6NuikRNaonSkhzrSMJIUQpiuqgNyk6depUtdfVYvxSzc3B/NxECG6G7smXURSlxtuUcVj7JG2xP47SDrDeOQu5gttOKB5eKA+MgoN7UXdt0zqOEEKUIsXCjih33QdNmqN+tgy1sEDrOEIIYSHFwo4oOid0wyZAZgbqus+1jiOEEBZSLOyM0ioMJfxu1HWrUc+lax1HCCEAKRZ2SRk0GpycMH+6TOsoQggBSLGwS4pfA5T7B8PuHagpu7WOI4QQUizslXLfAAgIwvzxu6hXrkkRQgitSLGwU4qzAd2QcXD6OOrGtVrHEULUc1Is7Fn726HdbahrPkLNztI6jRCiHpNiYccURbncuygqRF29Qus4Qoh6TIqFnVOCQlC690fdloR6+KDWcYQQ9ZQUizpA6TsEfPwwf7QY1WzWOo4Qoh6SYlEHKG7uKAMfgsMHUXds1DqOEKIekmJRRyiR0dCiDern76Pm52kdRwhRz0ixqCMUnQ7d0Ich5yLq16u0jiOEqGekWNQhSvNWKF1jUDf8D/X0Ca3jCCHqESkWdYzyj1FgcMG8agkO+twqIYQdkmJRxyjevij9h8Efu+G3n7WOI4SoJ6RY1EFKdB9o1ATzqndRi4u0jiOEqAekWNRBil6Pbug4OJeOuv4rreMIIeoBKRZ1lBLWCTpFoq79BDXTMR40L4SwXzYpFgsXLmTcuHE88cQT5S6XmprK0KFD2bFjh2VaRkYGs2bNYurUqUydOpWzZ89aO26doRs8Bsxm1M/f1zqKEMLB6W3xJtHR0fTq1YuEhIQbLmM2m1m5ciUdOnQoNf2tt95i4MCBtG/fnoKCAhRFsXbcOkMJCELpNRD161Wo0b1RWoVpHUkI4aBs0rMICwvD09Oz3GUSExOJiIjA29vbMu3EiROUlJTQvn17AFxdXXFxcbFq1rpG6TUI/I2YP3oH1VyidRwhhIOySc+iIpmZmezcuZO4uDgWLVpkmX7q1Ck8PDyYO3cuZ8+e5dZbb2XEiBHodNfXuKSkJJKSkgCYM2cORqOx2nn0en2N1re1gjGPc3Hu/+Gx+0fcez5gmV7X2lEeaYt9cpS2OEo7wHptsYtisXz58jKLgNlsZt++fbz22msYjUbeeOMNNm3axL333nvdNmJiYoiJibG8zsio/klfo9FYo/VtTW3dHlq3I+fDReTe3AHFwwuoe+0oj7TFPjlKWxylHVCztgQHB99wnl0Ui7S0NBYsWABAdnY2u3fvRqfT4e/vT2hoKIGBgQCEh4dz8ODBMotFfaYoCrph4zG/OBX1q/+iDJ+gdSQhhIOxi2Jx7YnvhIQEOnfuTHh4OGazmby8PLKzs/H29mbv3r20aNFCw6T2SwlpjhLdC3VTImpUT5SQUK0jCSEciE2Kxfz580lJSSEnJ4eJEycSGxuLyWQCoEePHjdcT6fTMWrUKF588UVUVaVFixalhppEacqAEag7t2D+aDG6J1/WOo4QwoEoqoPeje7UqVPVXrcuj1+aNyWirlyE8vBTNOz9QJ1tx9/V5c/k76Qt9sdR2gHWO2chV3A7GCWqBzRpjvrZMtSCfK3jCCEchBQLB6PonNANmwCZGeSu/lDrOEIIByHFwgEprcJQwu8m98uVqOfStY4jhHAAUiwclDJoNIqTE+ZPl2kdRQjhAKRYOCjFrwEegx6C3TtQU3ZrHUcIUcdJsXBg7v2GQEAQ5o/fRb3yVWUhhKiOSheLvXv3Wm4PnpWVxVtvvcXChQu5cOGCtbKJGlIMLuiGjIPTx1E3rdU6jhCiDqt0sVi6dKnl3k0ffPABJSUlKIrCO++8Y7Vwoha0vx3a3Yb6v49Qsy9onUYIUUdVulhkZmZiNBopKSnh119/ZcKECYwfP56DBw9aM5+oIUVRLvcuigpRv1ihdRwhRB1V6WLh5ubGhQsXSElJISQkBFdXVwDLbTuE/VKCQlC690fdloR6+E+t4wgh6qBKF4tevXoxY8YM3nzzTXr27AnA/v37ady4sdXCidqj9B0CXj6YP16MajZrHUcIUcdU+kaCDzzwAOHh4eh0OoKCggDw9/dn4sSJVgsnao/i5o7y4EOo7y1A3bEJ5U65zbsQovKq9NXZ4OBgS6HYu3cvFy5coGnTplYJJmqfEnkPNG+Nuvp91Pw8reMIIeqQSheLuLg49u/fD8CXX37JggULWLBgAatXr7ZaOFG7FJ3u8n2jLmahrl2ldRwhRB1S6WJx/PhxWrduDcCGDRuIi4vj5ZdfZv369VYLJ2qf0rwVStcY1KQ1qOkntI4jhKgjKl0srj72Ij398o3pQkJCMBqN5ObmWieZsBpl4CgwGDCvehcHfZyJEKKWVfoEd5s2bVi2bBlZWVncfvvtwOXC4eXlZbVwwjoUbz+UfsNQP1kKv+2CDrdrHUkIYecq3bOYNGkS7u7uNGvWjNjYWODy0+juv/9+q4UT1qPc0wcaNcG8aglqcbHWcYQQdq7SPQsvLy+GDx9eatptt91W64GEbSh6Pbqh4zC/EYea9BVK70FaRxJC2LFKFwuTycTq1avZvHkzWVlZ+Pn5ERUVxcCBA9HrK70ZYUeUsE7QKRJ17Seokfeg+DXQOpIQwk5Vehjqww8/5Pfff2f8+PHEx8czfvx49u7dy4cfyqM76zLd4DFQUoL62XKtowgh7FiluwQ7duwgPj7eckI7ODiY5s2bM336dEaPHl3uugsXLiQ5ORkfHx/mzZt3w+VSU1N57rnnmDJlCpGRkQAMGTLEcuGf0Wjk6aefrmxkUQlKQBBKr4GoX69Cje6N0ipM60hCCDtU6WJRk69YRkdH06tXLxISEm64jNlsZuXKlXTo0KHUdIPBQHx8fLXfW1RM6TUIdfsGzB+9g+6511F0TlpHEkLYmUoPQ91xxx28+uqr7NmzhxMnTrBnzx7i4+O54447Klw3LCwMT0/PcpdJTEwkIiICb2/vykYStURxcUEZNAaOH0bdIhdZCiGuV+mexciRI/n8889ZunQpWVlZ+Pv7c+edd9bKLcozMzPZuXMncXFxLFq0qNS84uJinnnmGZycnBgwYADh4eFlbiMpKYmkpCQA5syZg9ForHYevV5fo/XtRVXaofYaQNa29Zi+Wol/j/7ovOyraDvKZwLSFnvkKO0A67Wl0sVCr9czZMgQhgwZYplWVFTEqFGjGDlyZI1CLF++nBEjRliexHethQsX4u/vz5kzZ3jxxRdp2rSp5WaG14qJiSEmJsbyOiMjo9p5jEZjjda3F1VthzpoNOqLU8l47z/ohk+wYrKqc5TPBKQt9shR2gE1a0twcPAN59XoO6+KotRkdYu0tDQWLFgAQHZ2Nrt370an0xEeHo6/vz8AgYGBhIWFceTIkTKLhag5JaQ5SnQv1E2JqFE9UUJCtY4khLATdnGBxLUnvhMSEujcuTPh4eFcunQJFxcXnJ2dyc7O5sCBAwwYMEDDpI5PGTACdecWzB8vQffErFo7IBBC1G0VFou9e/fecF5lz1fMnz+flJQUcnJymDhxIrGxsZZ1e/ToccP1Tp48yeLFi9HpdJjNZh544AFCQkIq9Z6iehQPL5QHRqKuXAS/bIMu3bSOJISwAxUWi7+fcP67ypxImTJlSqUDTZo0yfJzmzZtyr0uQ1iHEtUD9Yd1mD9dhu7W21FcXLSOJITQWIXForxrI4RjUnRO6IY9jDl+Buq6z1EGDK94JSGEQ6vSY1VF/aG0bosSHoW67nPUc+laxxFCaEyKhbgh5cHRoNNh/uw9raMIITQmxULckOJvRLl/MCT/iJqyR+s4QggNSbEQ5VJ6PAABQZg/XoJaC1frCyHqJikWolyKswFd7Fg4fRx10zdaxxFCaESKhahYh3Bo2wn1fx+hZl/QOo0QQgNSLESFFEVBN2Q8FBWgfikPuxKiPpJiISpFaRSC0r0/6tb1qEf+1DqOEMLGpFiISlP6DgEvn8snu81mreMIIWxIioWoNMXNHeXBhyBtP+pPP2gdRwhhQ1IsRJUokfdA89aony9HLcjTOo4QwkakWIgqUXQ6dMMmwMUs1K9XaR1HCGEjUixElSnNW6F0jUFNWoOafkLrOEIIG5BiIapFGTgKDAbMq95FVVWt4wghrEyKhagWxdsPpd8w2JsMv+3SOo4Qwsrs4rGqom5S7umDuuU7zKuWoAvriOLsXCvbVVWVCwUlHL1QaPkX4JNNv5s88DQ41cp7CCGqRoqFqDZFr0c3dBzmN+JQk75C6T2oytvIKy7h2IWiy0Xh4l/FIaewxLKMj6sTOYcv8t1+PY+EB3F7iGdtNkMIUQlSLESNKGGdoGMk6tpPUCPvQfFrUOZyxSUqJ7P/KgbHrhSGs7l/3cnWVa+jma+BO5p40tTHhWa+l//5uOo5V+LCS4n7mfXDCaJDvRnXJRAvF+llCGErUixEjelix2B+ftLlay/GTuPspeJSPYVjFwo5mV1EyZXz4E4KhHi7cLPRnZ43udDU10AzXxcCPJzRKUqZ73FLoBfzeofy6R8ZfLb3PHvSc5l4exB3NPWyYUuFqL+kWIhqu5BvulIQDByNnsLRzHyOf7yfAvNfO/xAT2ea+rgQHuJl6SkEexlwdiq7KJTH2UlhePsAIkO8eHPHaeZsOUnXpl5MuD0QH1f5UxbCmmzyf9jChQtJTk7Gx8eHefPm3XC51NRUnnvuOaZMmUJkZKRlel5eHtOmTeP2229n7NixtogsrpFfbLYMG13tKRy9UMjFa88ruATSVDlM96y9hN7XnWZ+bjTxMeDuXPtDRS38XZnbK5TVKedZ9XsGv5/J4+EugXRr5oVyg56JEKJmbFIsoqOj6dWrFwkJCTdcxmw2s3LlSjp06HDdvFWrVnHLLbdYM6IATGaVk9lFpb6FdOxiIWcuFVuWcdUrNPFx4fYQT0tPoZmvC76uesw/n0FdnIDSzgtdq15WzarXKcS2M1p6GXO3nWLrMU8m3h6En5v0MoSobTb5vyosLIyzZ8+Wu0xiYiIRERGkpaWVmn7o0CEuXrxIx44dr5snqsesqpzLLb6mp3C5QJzMKcR05WayTgo09jbQqoErMS19aHblhHNDzxufV1C6dEPd9A3qFytQO3dD8bD+t5aa+rrwao9mfLUvk//+lsGjZw4xrnMg0c29pZchRC2yi0OwzMxMdu7cSVxcHIsWLbJMN5vNfPDBBzz22GP8/vvv5W4jKSmJpKQkAObMmYPRaKx2Hr1eX6P17YVer8fJ3YdD53M5dD6PtIzL/z10Po/84r+GkIK8XGhpdOeuVgG0bOBOS6MHTXzdMOirfs1m8cSnyHzyX7isX433uGm12pbyPpOHGwbQ89YmvJL0J/N/PM3O0wU81f0mAjxdai1DbXGUvy9wnLY4SjvAem2xi2KxfPlyRowYgU5Xeuf03Xff0alTJxo0KPvrmNeKiYkhJibG8jojI6PaeYxGY43W10KBycyxK8NGR670GE5kF5OZ99cQkpeLE818Xbi3hbelp9DUt6zzCvlkX8ivXhAvP5SoXuQnrqawSxRKSGi123StynwmHsCL0cGsPZjFij3nGP7BL4y5rSExLX3sqpdRF/++bsRR2uIo7YCatSU4OPiG8+yiWKSlpbFgwQIAsrOz2b17NzqdjoMHD7Jv3z6+++47CgoKMJlMuLq6MmLECI0Ta8dkVjmVU8TRrNLXK5y5VMzVOzS5OCk09XXhjlA/Al255ryCk012msqA4ag/b8H88RJ0T8yy6Y7aSafQ/2Z/bm/syX92nOatn9LZeiyHSeFBNPSsnSvMhaiP7KJYXHviOyEhgc6dOxMeHk54eLhl+qZNm0hLS6s3hUJVVc7lmkr1FI5dKOREdhEm8+WyoFMg2MtAS39X7m3hYykKgVfOK2h1tKR4eqM8MBJ15SJI3g6du9o8QyMvA7NimrLuzwu8v/ssj609zOhOAfRs5XvDcy5CiBuzSbGYP38+KSkp5OTkMHHiRGJjYzGZLl+526NHD1tEsGvZhSUcvVBgOdF85EphyDf99ejSAHc9TX1duC3Yw1IUGnsbMDjZ570glageqD+sw/zJMnTtuqC42P7cgU5RuL+1H52DPUj4KZ23fz7D1mM5PBYRRJCXweZ5hKjLFNVB7y996tSpaq9rrSPyQlPZ1ytkFfx1stnLoLtyLuHK11J9Lv/sUY0b6Gk9Dqse/ANz/AyUfkPR9R9eo23VtC2qqrI+7SLvJZ+lxKwyqmMAfdr4adLL0PpzqU2O0hZHaQc4+DkLR1Ny5bzCsQvXDCFdLCQ956/zCgany9crdAr2pJmvgWa+rjT1MeDvprerk7E1obRuixIehbpuNeqd3VGMgdplURR63ORLp0YeLNqZzru/nGXbsRwei2xEY2/pZQhRESkWNaCqKhl5plK9hKMXCzl+sfR5hUZeBpr7uRLd/K/rFQI9nXHSOUZRKI/y4GjUPT9h/vQ9nP7fM1rHIcDDmf+LDmHj4Wze/eUMU745zPD2Rvrf7F8vPg8hqkuKRSXlFJZYegrHrrlBXm7xX+cVGrjrCfV1oWPQX+cVQnzs97yCLSj+RpT7B6N++SHqvl9Rbrn+Cn2bZ1IU7m3hQ4cgd97++QzLd59j27EcJt/RiKY+9nddhhD2QIrF3xSazOw/c4nfjl4sdduLzPy/bqXtYdDRzMeFqFBvS1Fo6uOCp9wyu0xKjwdQtyVd/irt/81H0dvHn10Dd2dmRjVmy9EcFu86w9RvjjD01gb8I6wBeullCFGKffxfayfO5Rbz8FdpXBlBwlmn0MTHQIcg91L3QXKk8wq2oDgb0MWOxZzwMuqmb1Bi+msdyUJRFKJCvWkf6M47u87w4a8Z/Hg8h8mRjQj1c9U6nhB2Q4rFNRq46xlyq5G2IUb8nApp5GmQceza0iEc2nZC/d9HqOFRKN6+WicqxddNz9N3NWbbsWze+fkM0xKPMLhdAwa1NVbrdupCOJr6O5heBp2iMPRWI/e0MhLi7SKFohYpioJuyHgoKkD98kOt49xQ16bevNWnOV2befPx7+d5ct0RUs8XaB1LCM1JsRA2ozQKQeneD3XretQjf2od54a8XfU80TWYmXc35mJhCdO/PcKKPecoLjFXvLIQDkqKhbAppe9Q8PLB/PESVLN973wjQrx4q09zopv78Nkf55nyzREOZFTzBotC1HFSLIRNKW7uKAMfgrT9qD/9oHWcCnm6OPH4HY14PjqEfJOZZ747ynvJZyk02XehE6K2SbEQNqfccQ80b436+XLUgjyt41RK58ae/KdPc2Ja+vDlvkymfHOYlLN1I7sQtUGKhbA5RadDN+xhuJiF+vUnWsepNA+DE5MiGvHve5tgMsPM9cdYsusMBdLLEPWAFAuhCaV5a5Su3VGT/oeaflLrOFXSsZEHb/ZpTu/Wvnx9IIvJaw/zW3qu1rGEsCopFkIzysB/gsGA+ZOlWkepMjdnHRNuD+LlmKYowP9tOM6inenkXfO4WiEciRQLoRnF2+/yt6N+34X6289ax6mWdoHuvNmnOf1v9uPbPy8w+evD7D4tvQzheKRYCE0p9/aBoBDMq95FLS6ueAU75KLXMbZzILN7NMWg1/HC98f5z47TXCqSXoZwHFIshKYUvTO6YePh7GnUpP9pHadGbglwZ/79oQwM8+f7QxeZ/PVhdp28pHUsIWqFFAuhOSWsE3SMRF27CvXCea3j1IjBScdDnRryWs9meBh0vLTpBG9sP0VOofQyRN0mxULYBV3sGCgpQf38fa2j1IpWDdx4vXcose0asPlINo9+fYgfj+doHUuIapNiIeyCEhCE0vMfqDs2oaamaB2nVjg76RjRIYB5vULxc9MzZ/NJ4reeJCuvbp6bEfWbFAthN5Teg8DPiPmjJahmxxm2aeHvytxeoQxvb2TH8RxGfpjM1qPZqKpa8cpC2AmbFIuFCxcybtw4nnjiiXKXS01NZejQoezYsQOAc+fO8fTTTzN9+nSmTZvGd999Z4u4QiOKiyvK4H/BsTTUrUlax6lVep3CkFuNzOsVSpCXC/FbTzFny0myrnkCoxD2zCbFIjo6mpkzZ5a7jNlsZuXKlXTo8Nczmv38/Jg1axbx8fG88sorfPXVV2RmZlo7rtCQ0qUbtG6L+sUK1FzH+yZRqJ8r7wzpwD87BvDLyVwe+/oQmw5flF6GsHs2KRZhYWF4enqWu0xiYiIRERF4e3tbpun1epydnQEoLi7GbOe3tBY1pygKuqEPQ+4l1DUfaR3HKvQ6hQfbNmD+/aEEext4Y/tpXv7hBOflXIawY3bxWNXMzEx27txJXFwcixYtKjUvIyODOXPmkJ6ezsiRI/H39y9zG0lJSSQlXR66mDNnDkajsdR8VVXJzMzEZKq423/27FmHONKzVjv0ej3+/v7Wew650Uh2zwfI/+4rfPvFom/WEr1ef91nWlddbYvRCEuaB/PpnlMs/vEok9ce4bGo5vQJC6wzz3h3lM/FUdoB1muLXRSL5cuXM2LECHS66zs6RqORuXPnkpmZSXx8PJGRkfj6+l63XExMDDExMZbXGRkZpebn5+fj7OyMXl9xk/V6faWKir2zVjuKi4s5ceIEbm5utb7tq9SeA2HLes6/HY9u2ksEBARc95nWVUajsVRbYpq6EOYbyls/nWZ2UiqJf5zm0YggAjycNUxZOX9vS13lKO2AmrUlODj4hvPs4ttQaWlpLFiwgEmTJrFjxw7effdddu7cWWoZf39/mjRpwv79+6v1HmazuVKFQlRMr9dbfUhQ8fRGeWAE7P8Nkrdb9b3sQbC3gVkxTXm4SyD7z+Xx6NeHSTyYhdkBerjCMdjF3jMhIaHUz507dyY8PJzz58/j5eWFwWDg0qVLHDhwgL59+1brPepKt76usMXvU4nqifrDt5g/WYYa3dPq76c1naLQp40fXRp78NaOdN7++QzbjuXwaEQQQV4GreOJes4mxWL+/PmkpKSQk5PDxIkTiY2NtQyP9OjR44brnTx5kg8++ABFUVBVlX79+tG0aVNbRBZ2QNE5oRs2HnP8TLIXvYq5cXOtI9WKPE8PzJdufGfaAOAFHaz38WL52QZMXpPKKK9Mentko7OzY56K2lJXOEo7AApCmkCrW2t9u4rqCGdyy3Dq1KlSr/Py8nB3d6/UunLOomJV+X3WlPmDt1C31M9rbDJcfFjU+kF2N7iZWy4cZtKBTwnOd4yxdWEdzq3bYp4+u1rrlnfOQopFGay1k7148SJffPEFo0ePrtJ6o0aN4q233sLHx6dK602bNo1777232kN35bFlsVBVlQYGPefP1+2bDF7VwN+f81W4XkhVVb4/kc/SvTmYzCrDb/aiXwt3nOxgaLWqbbFXjtIOgAYBDcksLKrWuuUVC7s4Z2Fr5o+XoB4/fOP5V4a9qkJp0hzd0PHlLpOdnc0HH3xwXbEwmUzlnnxfsWJFlbI4GkVR0Pn4oTjIU+h0vv4oVXhutwLE+PjRqXkxi3aeYXlKDtvPFPPYHY1o6uNivaCVUNW22CtHaQeAzssbCmu/91kvi4VWXnnlFY4ePcp9992Hs7MzLi4u+Pj4kJqaytatWxkzZgynTp2isLCQsWPHMnLkSAAiIiJITEwkNzeXkSNHEh4ezq5duwgKCmLZsmWV+grrli1beOmllygpKaFDhw7Mnj0bFxcXXnnlFb777jv0ej1RUVE8//zzrFmzhjfeeAOdToe3tzerV6+29q9GVEIDd2eevbsxm49ks2TXGaZ+c4Rhtxr5R5g/TvZ2MkM4nHpZLCrqAVhrGGrmzJkcOHCA9evXs337dv75z3/y/fffW07az5s3Dz8/P/Lz8+nTpw/333//dRchHj58mISEBOLj45kwYQLffPMNDz74YLnvW1BQwNSpU1m1ahUtW7Zk8uTJfPDBBzz44IMkJiayefNmFEXh4sWLwOUvJKxcuZJGjRpZpgn7oCgKdzf3oUOQB2//fIYVv55j+/EcJkcGEernqnU84cDs4jqL+qpjx46lvt21bNkyYmJi6NevH6dOneLw4euHypo0aUK7du0AaN++PcePH6/wfdLS0mjatCktW7YEYPDgwfz00094e3vj4uLCE088wTfffGPpoXTp0oWpU6eycuVKSkocY+jH0fi66XkmqjFP3RVMRm4xT6w7wse/Z1Bc4pCnIIUdkGKhoWtPEG/fvp0tW7awZs0akpKSaNeuHYWFhdet4+Ly1xi1k5NTjXbmer2etWvX0qdPH5KSkhgxYgQAr776Kk899RSnTp2id+/ecvNGO9a1qTdv9W3OnU28+ei3DJ5cd4S0zAKtYwkHJMXChjw8PLh0qew7qebk5ODj44ObmxupqakkJyfX2vu2bNmS48ePW3oqn3/+OZGRkeTm5pKTk0P37t154YUXSEm5/NChI0eOcNtttzF9+nQaNGhw3TfLhH3xdtXzRLdgZkY15mKBiSfXHeHDPecoLnGME7bCPtTLcxZa8ff35/bbb+fee+/F1dW11M2+oqOjWbFiBXfffTctW7bktttuq7X3dXV15fXXX2fChAmWE9yjRo3iwoULjBkzhsLCQlRVJS4uDoBZs2Zx+PBhVFWlW7dutG3bttayCOuJaOJF24buLE0+w6d/nGfHiRwmRzaitdF69/AS9YdcZ1EGuSivYra8zgLkRm9VtevkJRb+lE5WgYkBN/szrL0RF33tDyQ4yufiKO0AB7+RoBCidnVp7Ml/+jYnpqUPX+zLZMo3R9h3Nk/rWKIOk2EoBzBz5kx+/vnnUtPGjRtnOWEt6icPgxOTIhrRtak3CT+dZsb6Y/Rt48fIjgG4WqGXIRybFAsH8Morr2gdQdixjo08WNCnOR/sPseaA1n8fPISj0YGcWugh9bRRB0ihxdC1APuzk5MDA9iVkwTAJ5LOs7bO9PJc5BbqAjrk2IhRD1ya+DlXka/m/1Y9+cFHl97mD2nHePW3MK6pFgIUc+46nWM6xzI7PuaotfpiPv+OG/tOE1ukfQyxI1JsRCinrqloTvz7w9lYJg/Gw5d5LGvD7PrZNkXjQohxcKOtWrV6obzjh8/zr333mvDNMIRueh1PNSpIa/2aIa7QcdLm04wf/spcgqllyFKq5ffhnp31xkOZ934/jlKNZ5n0dzPlXFdAmsaTQhNtDa68UbvUD7Ze57P/jjPntO5TAwPIrKJl9bRRDlKzCoFJjP5JjN5xWbyi81klFzC6FT771Uvi4VWXnnlFYKDgy0PP5o3bx5OTk5s376dixcvYjKZeOqpp+jZs2eVtltQUMCMGTP47bffcHJyIi4ujq5du7J//34ef/xxioqKUFWVxYsXExQUxIQJEzh9+jRms5nHH3+cAQMGWKG1oq5xdtIxokMAdzTx4s0dp5m9+SR3NfPi4S6BeLvKrqK2qKpKgUm9soMvIf/KTj7vb/+9Ot/y+uo8S2EoocB0/UFtWFAms7s3rvXc9fIvoKIegLVuk9G/f3/i4uIsxWLNmjWsXLmSsWPH4uXlRWZmJv369aNHjx4oVXhk5vLly1EUhQ0bNpCamsqwYcPYsmUL77//PmPHjmXgwIEUFRVRUlLC999/T1BQkOXpe9nZ2bXeTlG3tfB3Jb5nKJ+nnOfTvRn8lp7HhNsD6drMW+tomlFVlWKzet2O++rOvvQO/tqdfollWl6xmYIry5grMXDhpIC7sw43Zx1uzk64O+vwdnEi0NMZN2cd7lf+Xf7ZCTf95Z9Dg4xA7d952CbFYuHChSQnJ+Pj48O8efNuuFxqairPPfccU6ZMITIykiNHjrBkyRLy8/PR6XQMHDiQO++80xaRraJdu3ZkZGSQnp7O+fPn8fHxoWHDhrzwwgv89NNPKIpCeno6586do2HDhpXe7s8//8y//vUvAG666SZCQkI4dOgQXbp0Yf78+Zw+fZrevXvTokULbr75Zl588UVefvllYmJiiIiIsFZzRR3m7KQw9FYjkSGevLkjnde2nuKOozlMvD0QX7e6c4xpsuzgS64/ejdd/bkEVZ9NZnZumTv9/CtH95V5VIjC5R2869Uduf7yf/3d/raD1+ssry07+2teu+l1GJyUKh00XmU0epKRUUeLRXR0NL169SIhIeGGy5jNZlauXEmHDh0s0wwGA48++iiNGjUiMzOTZ555hg4dOuDhUXevPO3bty9r167l7Nmz9O/fn9WrV3P+/HkSExNxdnYmIiKizOdYVMeDDz5Ihw4d2LBhA6NGjeLVV1+lW7durFu3ju+//57XXnuNbt26MXXq1Fp5P+F4Qv1cie/ZjC/2ZfLRbxnsPZPLuC6B3B3qXa0dWWWUmFXLjrz00Et5QzYllqP3a5cprswhPODmrMNVX3oHH3TNEfzlaU5XjvJLH9Vfe2Tvqq/eDr4usEmxCAsL4+zZs+Uuk5iYSEREBGlpaZZp194B0d/fHx8fH7Kzs+t0sejfvz/Tp08nMzOTzz//nDVr1mA0GnF2dmbbtm2cOHGiytsMDw/niy++oFu3bqSlpXHy5ElatmzJkSNHaNasGWPHjuXkyZPs27ePm266CV9fXx588EG8vb356KOPrNBK4UicdAqD2jYgIsST/+w4zRvbT7P1aA7/LzyQqzfZvzoOb9mhXzP08tfOu6SMo/rrh2zKGocvi8FJsRyFX915G931liGbq9NL7+D/2uFfne+q1xHYMMBh7jprLXbRn8zMzGTnzp3ExcWxaNGiMpdJTU3FZDIRGFi3v3HUpk0bcnNzCQoKIjAwkIEDB/LQQw/RvXt32rdvz0033VTlbT700EPMmDGD7t274+TkxBtvvIGLiwv/+9//+PTTT9Hr9TRs2JDHHnuMX3/9lVmzZqEoCs7OzsyePdsKrRSOqImPC7Pva8bXB7L48NdzPLLmEN6ux7lUaCK/2ExldvFOCrgbnErt4H1cnQjycr7hEfz1O/3L8/U6xzyCt1c2e57F2bNnefXVV8s8Z/H666/Tt29fWrduTUJCAp07dyYyMtIyPysrixdeeIFJkybRunXrMreflJREUlISAHPmzKGoqKjU/DNnzpR6JKmomcLCQpsWbkd5xgg4RluOZ+Wz8pcTqFw+uvcwOOFucMLD4ISHQY+7sxMeLpenuTtfmWZwqvY4vLU5wmdyVU3aYjAYbrzd6gaqTWlpaSxYsAC4/O2c3bt3o9PpCA8PJy8vjzlz5jBs2LAbFgqAmJgYYmJiLK//3qUsLCzEyalyXz52lD8ca7ajsLDQpt12eTiNfXEDxnX0K6ct5iv/isEEJSbIsePHaTjCZ3KVtR5+ZBfF4toT31d7FuHh4ZhMJubOnUtUVFSpnkZ9sm/fPiZPnlxqmouLC19//bVGiYQQ9ZFNisX8+fNJSUkhJyeHiRMnEhsbazni7dGjxw3X2759O/v27SMnJ4dNmzYBMGnSJEJDQ6ucoa4+PfaWW25h/fr1Wse4Tl39fQohqqfePIM7Pz8fZ2dn9PqK66MMQ5XPZDJRXFyMm5tbrW/7RmSYwD45SlscpR3g4MNQtuDq6kpBQQGFhYUVnmBzcXGptWsdtGSNdqiqik6nw9XVtVa3K4Swb/WmWCiKUukjYUc5ynCUdgghtCe3KBdCCFEhKRZCCCEqJMVCCCFEhRz221BCCCFqj/QsyvDMM89oHaFWOEo7QNpirxylLY7SDrBeW6RYCCGEqJAUCyGEEBWSYlGGa29IWJc5SjtA2mKvHKUtjtIOsF5b5AS3EEKICknPQgghRIWkWAghhKhQvbk31N/t2bOH9957D7PZTPfu3XnggQdKzS8uLuatt97i0KFDeHl5MWXKFBo2bKhN2ApU1JZNmzaxYsUK/P39AejVqxfdu3fXIGn5Fi5cSHJyMj4+PmU+UVFVVd577z12796Ni4sLjzzyCC1atNAgacUqassff/zBa6+9ZvmbioiIYNCgQbaOWaGMjAwSEhK4cOECiqIQExPD/fffX2qZuvK5VKYtdeVzKSoqIi4uDpPJRElJCZGRkcTGxpZaptb3YWo9VFJSoj766KNqenq6WlxcrD755JPq8ePHSy2zbt069Z133lFVVVW3bt2qvv7661pErVBl2rJx40b13Xff1Shh5f3xxx9qWlqaOm3atDLn//LLL+rLL7+sms1m9cCBA+qMGTNsnLDyKmrL3r171dmzZ9s4VdVlZmaqaWlpqqqqal5enjp58uTr/r7qyudSmbbUlc/FbDar+fn5qqqqanFxsTpjxgz1wIEDpZap7X1YvRyGSk1NJSgoiMDAQPR6PXfeeSc///xzqWV27dpFdHQ0AJGRkezdu9cuH/hTmbbUFWFhYXh6et5w/q5du4iKikJRFFq3bk1ubi5ZWVk2TFh5FbWlrvDz87P0Etzc3GjcuDGZmZmllqkrn0tl2lJXKIpieUxASUkJJSUl1z16obb3YfVyGCozM5MGDRpYXjdo0IA///zzhss4OTnh7u5OTk4O3t7eNs1akcq0BeCnn35i3759NGrUiIceegij0WjLmLUiMzOzVO4GDRqQmZmJn5+fhqmq7+DBg0yfPh0/Pz9GjRpFkyZNtI5UrrNnz3L48GFuuummUtPr4udyo7ZA3flczGYzTz/9NOnp6fTs2ZNWrVqVml/b+7B6WSzqm86dO9O1a1ecnZ1Zv349CQkJxMXFaR2rXmvevDkLFy7E1dWV5ORk4uPjefPNN7WOdUMFBQXMmzeP0aNH4+7urnWcGimvLXXpc9HpdMTHx5Obm8vcuXM5duwYTZs2td77WW3Ldszf35/z589bXp8/f95y8resZUpKSsjLy8PLy8umOSujMm3x8vLC2dkZgO7du3Po0CGbZqwt/v7+pR7mVFZb6wp3d3fLMMJtt91GSUkJ2dnZGqcqm8lkYt68edx1111ERERcN78ufS4VtaUufS5XeXh40LZtW/bs2VNqem3vw+plsWjZsiWnT5/m7NmzmEwmtm/fTpcuXUot07lzZzZt2gTAjh07aNu2bYWPY9VCZdpy7fjxrl27CAkJsXXMWtGlSxc2b96MqqocPHgQd3d3ux7qKM+FCxcs48epqamYzWa7PBhRVZW3336bxo0b07dv3zKXqSufS2XaUlc+l+zsbHJzc4HL34z67bffaNy4callansfVm+v4E5OTub999/HbDZzzz33MHDgQFatWkXLli3p0qULRUVFvPXWWxw+fBhPT0+mTJlCYGCg1rHLVFFb/vvf/7Jr1y6cnJzw9PRk3Lhx1/1h2YP58+eTkpJCTk4OPj4+xMbGYjKZAOjRoweqqrJ06VJ+/fVXDAYDjzzyCC1bttQ4ddkqasu6dev47rvvcHJywmAw8M9//pM2bdponPp6+/fv5/nnn6dp06aWHc2wYcMsPYm69LlUpi115XM5evQoCQkJmM1mVFXljjvuYNCgQVbdh9XbYiGEEKLy6uUwlBBCiKqRYiGEEKJCUiyEEEJUSIqFEEKICkmxEEIIUSEpFkLYgdjYWNLT07WOIcQNye0+hPibSZMmceHCBXS6v46loqOjGTt2rIapyvbtt99y/vx5hg8fTlxcHGPGjKFZs2ZaxxIOSIqFEGV4+umnad++vdYxKnTo0CFuu+02zGYzJ0+erLNX5wv7J8VCiCrYtGkTGzZsIDQ0lM2bN+Pn58fYsWO59dZbgct3+lyyZAn79+/H09OTAQMGEBMTA1y+S+iXX37Jxo0buXjxIo0aNWL69OmWO7b+9ttvvPLKK2RnZ9OtWzfGjh1b4e0ZDh06xKBBgzh16hQBAQE4OTlZ9xcg6i0pFkJU0Z9//klERARLly5l586dzJ07l4SEBDw9PVmwYAFNmjThnXfe4dSpU7z00ksEBQXRrl07vv76a7Zt28aMGTNo1KgRR48excXFxbLd5ORkZs+eTX5+Pk8//TRdunShY8eO171/cXEx48ePR1VVCgoKmD59OiaTCbPZzOjRo+nfvz8DBw604W9E1AdSLIQoQ3x8fKmj9JEjR1p6CD4+PvTp0wdFUbjzzjtZs2YNycnJhIWFsX//fp555hkMBgOhoaF0796dH374gXbt2rFhwwZGjhxJcHAwAKGhoaXe84EHHsDDw8NyF9EjR46UWSycnZ1Zvnw5GzZs4Pjx44wePZpZs2YxdOjQMp/PIERtkGIhRBmmT59+w3MW/v7+pYaHAgICyMzMJCsrC09PT9zc3CzzjEYjaWlpwOVbd5d3IzdfX1/Lzy4uLhQUFJS53Pz589mzZw+FhYU4OzuzceNGCgoKSE1NpVGjRsyePbsqTRWiUqRYCFFFmZmZqKpqKRgZGRl06dIFPz8/Ll26RH5+vqVgZGRkWJ7t0KBBA86cOVPjB9RMmTIFs9nMww8/zOLFi/nll1/48ccfmTx5cs0aJkQ55DoLIaro4sWLJCYmYjKZ+PHHHzl58iSdOnXCaDTSpk0b/vvf/1JUVMTRo0fZuHEjd911F3D5wVOrVq3i9OnTqKrK0aNHycnJqVaGkydPEhgYiE6n4/Dhw3Z5S3DhWKRnIUQZXn311VLXWbRv357p06cD0KpVK06fPs3YsWPx9fVl2rRplgfkPP744yxZsoQJEybg6enJ4MGDLcNZffv2pbi4mFmzZpGTk0Pjxo158sknq5Xv0KFDNG/e3PLzgAEDatJcISokz7MQogqufnX2pZde0jqKEDYlw1BCCCEqJMVCCCFEhWQYSgghRIWkZyGEEKJCUiyEEEJUSIqFEEKICkmxEEIIUSEpFkIIISr0/wFtxYTi/aXxiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model training loss and validation loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot-%d.png\" % target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-banking",
   "metadata": {},
   "source": [
    "Because the training of our encoder-decoder model did not yield such good results, we will be using a pretrained autoencoder to show images and their reconstructions. The autoencoder was taken from Yihao Wang's code. [2] The autoencoder shown below is taken from his work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19AutoEncoder(self, files_path):\n",
    "    #Load Full Model with every trained decoder\n",
    "\n",
    "\n",
    "    #Get Each SubModel\n",
    "    # Each model has an encoder, a decoder, and an extra output convolution\n",
    "    # that converts the upsampled activations into output images\n",
    "\n",
    "    # DO NOT load models four and five because they are not great auto encoders\n",
    "    # and therefore will cause weird artifacts when used for style transfer \n",
    "\n",
    "    ModelBlock3 = tf.keras.models.load_model(str(PurePath(files_path, 'Block3_Model')), compile = False)\n",
    "    self.E3 = ModelBlock3.layers[0] # VGG encoder\n",
    "    self.D3 = ModelBlock3.layers[1] # Trained decoder from VGG\n",
    "    self.O3 = ModelBlock3.layers[2] # Conv layer to get to three channels, RGB image\n",
    "\n",
    "    ModelBlock2 = tf.keras.models.load_model(str(PurePath(files_path, 'Block2_Model')), compile = False)\n",
    "    self.E2 = ModelBlock2.layers[0] # VGG encoder\n",
    "    self.D2 = ModelBlock2.layers[1] # Trained decoder from VGG\n",
    "    self.O2 = ModelBlock2.layers[2] # Conv layer to get to three channels, RGB image\n",
    "\n",
    "    # no special decoder for this one becasue VGG first layer has\n",
    "    # no downsampling. So the decoder is just a convolution \n",
    "    ModelBlock1 = tf.keras.models.load_model(str(PurePath(files_path, 'Block1_Model')), compile = False)\n",
    "    self.E1 = ModelBlock1.layers[0] # VGG encoder, one layer\n",
    "    self.O1 = ModelBlock1.layers[1] # Conv layer to get to three channels, RGB image\n",
    "\n",
    "def call(self, image, alphas=None, training  = False):\n",
    "    \n",
    "    # Input should be dictionary with 'style' and 'content' keys\n",
    "    # {'style':style_image, 'content':content_image}\n",
    "    # value in each should be a 4D Tensor,: (batch, i,j, channel)\n",
    "\n",
    "    style_image = image['style']\n",
    "    content_image = image['content']\n",
    "\n",
    "    output_dict = dict()\n",
    "    # this will be the output, where each value is a styled \n",
    "    # version of the image at layer 1, 2, and 3. So each key in the \n",
    "    # dictionary corresponds to layer1, layer2, and layer3.\n",
    "    # we also give back the reconstructed image from the auto encoder\n",
    "    # so each value in the dict is a tuple (styled, reconstructed)\n",
    "\n",
    "    x = content_image\n",
    "    # choose covariance function\n",
    "    # covariance is more stable, but signal will work for very small images\n",
    "    wct = self.wct_from_cov \n",
    "\n",
    "    if alphas==None:\n",
    "        alphas = {'layer3':0.6, \n",
    "                  'layer2':0.6, \n",
    "                  'layer1':0.6}\n",
    "\n",
    "    # ------Layer 3----------\n",
    "    # apply whiten/color on layer 3 from the original image\n",
    "    # get activations\n",
    "    a_c = self.E3(tf.constant(x))\n",
    "    a_s = self.E3(tf.constant(style_image))\n",
    "    # swap grammian of activations, blended with original\n",
    "    x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer3'])\n",
    "    # decode the new style\n",
    "    x = self.O3(self.D3(x))\n",
    "    x = self.enhance_contrast(x)\n",
    "    # get reconstruction\n",
    "    reconst3 = self.O3(self.D3(self.E3(tf.constant(content_image))))\n",
    "    # save off the styled and reconstructed images for display\n",
    "    blended3 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "    reconst3 = tf.clip_by_value(tf.squeeze(reconst3), 0, 1)\n",
    "    output_dict['layer3'] = (blended3, reconst3)\n",
    "\n",
    "    # ------Layer 2----------\n",
    "    # apply whiten/color on layer 2 from the already blended image\n",
    "    # get activations\n",
    "    a_c = self.E2(tf.constant(x))\n",
    "    a_s = self.E2(tf.constant(style_image))\n",
    "    # swap grammian of activations, blended with original\n",
    "    x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer2'])\n",
    "    # decode the new style\n",
    "    x = self.O2(self.D2(x))\n",
    "    x = self.enhance_contrast(x,1.3)\n",
    "    # get reconstruction\n",
    "    reconst2 = self.O2(self.D2(self.E2(tf.constant(content_image))))\n",
    "    # save off the styled and reconstructed images for display\n",
    "    blended2 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "    reconst2 = tf.clip_by_value(tf.squeeze(reconst2), 0, 1)\n",
    "    output_dict['layer2'] = (blended2, reconst2)\n",
    "\n",
    "    # ------Layer 1----------\n",
    "    # apply whiten/color on layer 1 from the already blended image\n",
    "    # get activations\n",
    "    a_c = self.E1(tf.constant(x))\n",
    "    a_s = self.E1(tf.constant(style_image))\n",
    "    # swap grammian of activations, blended with original\n",
    "    x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer1'])\n",
    "    # decode the new style\n",
    "    x = self.O1(x)\n",
    "    x = self.enhance_contrast(x,1.2)\n",
    "    # get reconstruction\n",
    "    reconst1 = self.O1(self.E1(tf.constant(content_image)))\n",
    "    # save off the styled and reconstructed images for display\n",
    "    blended1 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "    reconst1 = tf.clip_by_value(tf.squeeze(reconst1), 0, 1)\n",
    "    output_dict['layer1'] = (blended1, reconst1)\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, files_path):\n",
    "        super(VGG19AutoEncoder, self).__init__()\n",
    "        #Load Full Model with every trained decoder\n",
    "        \n",
    "        \n",
    "        #Get Each SubModel\n",
    "        # Each model has an encoder, a decoder, and an extra output convolution\n",
    "        # that converts the upsampled activations into output images\n",
    "        \n",
    "        # DO NOT load models four and five because they are not great auto encoders\n",
    "        # and therefore will cause weird artifacts when used for style transfer \n",
    "        \n",
    "        ModelBlock3 = tf.keras.models.load_model(str(PurePath(files_path, 'Block3_Model')), compile = False)\n",
    "        self.E3 = ModelBlock3.layers[0] # VGG encoder\n",
    "        self.D3 = ModelBlock3.layers[1] # Trained decoder from VGG\n",
    "        self.O3 = ModelBlock3.layers[2] # Conv layer to get to three channels, RGB image\n",
    "        \n",
    "        ModelBlock2 = tf.keras.models.load_model(str(PurePath(files_path, 'Block2_Model')), compile = False)\n",
    "        self.E2 = ModelBlock2.layers[0] # VGG encoder\n",
    "        self.D2 = ModelBlock2.layers[1] # Trained decoder from VGG\n",
    "        self.O2 = ModelBlock2.layers[2] # Conv layer to get to three channels, RGB image\n",
    "        \n",
    "        # no special decoder for this one becasue VGG first layer has\n",
    "        # no downsampling. So the decoder is just a convolution \n",
    "        ModelBlock1 = tf.keras.models.load_model(str(PurePath(files_path, 'Block1_Model')), compile = False)\n",
    "        self.E1 = ModelBlock1.layers[0] # VGG encoder, one layer\n",
    "        self.O1 = ModelBlock1.layers[1] # Conv layer to get to three channels, RGB image\n",
    "        \n",
    "\n",
    "    def call(self, image, alphas=None, training  = False):\n",
    "        # Input should be dictionary with 'style' and 'content' keys\n",
    "        # {'style':style_image, 'content':content_image}\n",
    "        # value in each should be a 4D Tensor,: (batch, i,j, channel)\n",
    "        \n",
    "        style_image = image['style']\n",
    "        content_image = image['content']\n",
    "        \n",
    "        output_dict = dict()\n",
    "        # this will be the output, where each value is a styled \n",
    "        # version of the image at layer 1, 2, and 3. So each key in the \n",
    "        # dictionary corresponds to layer1, layer2, and layer3.\n",
    "        # we also give back the reconstructed image from the auto encoder\n",
    "        # so each value in the dict is a tuple (styled, reconstructed)\n",
    "        \n",
    "        x = content_image\n",
    "        # choose covariance function\n",
    "        # covariance is more stable, but signal will work for very small images\n",
    "        wct = self.wct_from_cov \n",
    "        \n",
    "        if alphas==None:\n",
    "            alphas = {'layer3':0.6, \n",
    "                      'layer2':0.6, \n",
    "                      'layer1':0.6}\n",
    "        \n",
    "        # ------Layer 3----------\n",
    "        # apply whiten/color on layer 3 from the original image\n",
    "        # get activations\n",
    "        a_c = self.E3(tf.constant(x))\n",
    "        a_s = self.E3(tf.constant(style_image))\n",
    "        # swap grammian of activations, blended with original\n",
    "        x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer3'])\n",
    "        # decode the new style\n",
    "        x = self.O3(self.D3(x))\n",
    "        x = self.enhance_contrast(x)\n",
    "        # get reconstruction\n",
    "        reconst3 = self.O3(self.D3(self.E3(tf.constant(content_image))))\n",
    "        # save off the styled and reconstructed images for display\n",
    "        blended3 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "        reconst3 = tf.clip_by_value(tf.squeeze(reconst3), 0, 1)\n",
    "        output_dict['layer3'] = (blended3, reconst3)\n",
    "        \n",
    "        # ------Layer 2----------\n",
    "        # apply whiten/color on layer 2 from the already blended image\n",
    "        # get activations\n",
    "        a_c = self.E2(tf.constant(x))\n",
    "        a_s = self.E2(tf.constant(style_image))\n",
    "        # swap grammian of activations, blended with original\n",
    "        x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer2'])\n",
    "        # decode the new style\n",
    "        x = self.O2(self.D2(x))\n",
    "        x = self.enhance_contrast(x,1.3)\n",
    "        # get reconstruction\n",
    "        reconst2 = self.O2(self.D2(self.E2(tf.constant(content_image))))\n",
    "        # save off the styled and reconstructed images for display\n",
    "        blended2 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "        reconst2 = tf.clip_by_value(tf.squeeze(reconst2), 0, 1)\n",
    "        output_dict['layer2'] = (blended2, reconst2)\n",
    "        \n",
    "        # ------Layer 1----------\n",
    "        # apply whiten/color on layer 1 from the already blended image\n",
    "        # get activations\n",
    "        a_c = self.E1(tf.constant(x))\n",
    "        a_s = self.E1(tf.constant(style_image))\n",
    "        # swap grammian of activations, blended with original\n",
    "        x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer1'])\n",
    "        # decode the new style\n",
    "        x = self.O1(x)\n",
    "        x = self.enhance_contrast(x,1.2)\n",
    "        # get reconstruction\n",
    "        reconst1 = self.O1(self.E1(tf.constant(content_image)))\n",
    "        # save off the styled and reconstructed images for display\n",
    "        blended1 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "        reconst1 = tf.clip_by_value(tf.squeeze(reconst1), 0, 1)\n",
    "        output_dict['layer1'] = (blended1, reconst1)\n",
    "           \n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def enhance_contrast(image, factor=1.25):\n",
    "        return tf.image.adjust_contrast(image,factor)\n",
    "        \n",
    "    @staticmethod\n",
    "    def wct_from_cov(content, style, alpha=0.6, eps=1e-5):\n",
    "        '''\n",
    "        https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "           Perform Whiten-Color Transform on feature maps using numpy\n",
    "           See p.4 of the Universal Style Transfer paper for equations:\n",
    "           https://arxiv.org/pdf/1705.08086.pdf\n",
    "        '''\n",
    "        # 1xHxWxC -> CxHxW\n",
    "        content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "        style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "        # CxHxW -> CxH*W\n",
    "        content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "        style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "        # applt a threshold for only the largets eigen values\n",
    "        eigen_val_thresh = 1e-5\n",
    "        \n",
    "        # ===Whitening transform===\n",
    "        # 1. take mean of each channel\n",
    "        mc = content_flat.mean(axis=1, keepdims=True)\n",
    "        fc = content_flat - mc\n",
    "        # 2. get covariance of content, take SVD\n",
    "        cov_c = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
    "        Uc, Sc, _ = np.linalg.svd(cov_c)\n",
    "        # 3. truncate the SVD to only the largest eigen values\n",
    "        k_c = (Sc > eigen_val_thresh).sum()\n",
    "        Dc = np.diag((Sc[:k_c]+eps)**-0.5)\n",
    "        Uc = Uc[:,:k_c]\n",
    "        # 4. Now make a whitened content image\n",
    "        fc_white = (Uc @ Dc @ Uc.T) @ fc\n",
    "\n",
    "        # ===Coloring transform===\n",
    "        # 1. take mean of each channel\n",
    "        ms = style_flat.mean(axis=1, keepdims=True)\n",
    "        fs = style_flat - ms\n",
    "        # 2. get covariance of style, take SVD\n",
    "        cov_s = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
    "        Us, Ss, _ = np.linalg.svd(cov_s)\n",
    "        # 3. truncate the SVD to only the largest eigen values\n",
    "        k_s = (Ss > eigen_val_thresh).sum()\n",
    "        Ds = np.sqrt(np.diag(Ss[:k_s]+eps))\n",
    "        Us = Us[:,:k_s]\n",
    "        # 4. Now make a colored image that mixes the Grammian of the style\n",
    "        #   with the whitened content image\n",
    "        fcs_hat = (Us @ Ds @ Us.T) @ fc_white\n",
    "        fcs_hat = fcs_hat + ms # add style mean back to each channel\n",
    "\n",
    "        # Blend transform features with original features\n",
    "        blended = alpha*fcs_hat + (1 - alpha)*(content_flat) \n",
    "\n",
    "        # CxH*W -> CxHxW\n",
    "        blended = blended.reshape(content_t.shape)\n",
    "        # CxHxW -> 1xHxWxC\n",
    "        blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "        return np.float32(blended)\n",
    "\n",
    "    @staticmethod\n",
    "    def wct_from_signal(content, style, alpha=0.6 ):\n",
    "        # This uses a more computational SVD decomposition to get the Grammian\n",
    "        # to match. However, the numerical precision makes this totally fail\n",
    "        # if the activations are too large. \n",
    "        \n",
    "        # 1xHxWxC -> CxHxW\n",
    "        content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "        style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "        # CxHxW -> Cx(H*W)\n",
    "        content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "        style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "        singular_val_thresh = 1e-3\n",
    "        #-------------------------------------------\n",
    "        # Whitening transform and Coloring transform\n",
    "        # 1. SVD of content signals\n",
    "        mc = content_flat.mean()\n",
    "        fc = content_flat - mc\n",
    "        Uc, Sc, Vc = np.linalg.svd(fc, full_matrices=False)\n",
    "        k_c = (Sc > singular_val_thresh).sum()\n",
    "        \n",
    "        # 2. SVD of style signals\n",
    "        ms = style_flat.mean()\n",
    "        fs = style_flat - ms\n",
    "        Us, Ss, Vs = np.linalg.svd(fs, full_matrices=False)\n",
    "        k_s = (Ss > singular_val_thresh).sum()\n",
    "        \n",
    "        k = min(k_s,k_c)\n",
    "\n",
    "        # Blend transform features with original features\n",
    "        fcs = (Us[:,:k] @ np.diag(Ss[:k]) @ Vc[:k,:]) + mc\n",
    "        blended = alpha*fcs + (1 - alpha)*(content_flat)\n",
    "\n",
    "        # CxH*W -> CxHxW\n",
    "        blended = blended.reshape(content_t.shape)\n",
    "        # CxHxW -> 1xHxWxC\n",
    "        blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "        return np.float32(blended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-walnut",
   "metadata": {},
   "source": [
    "Now that we have the trained autoencoder, the model can be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-festival",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we make the autoencoder\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title==None:\n",
    "        title = str(image.shape)\n",
    "    else:\n",
    "        title += ' '+str(image.shape)\n",
    "    plt.title(title)\n",
    "    \n",
    "AE = VGG19AutoEncoder('models/vgg_decoder/')\n",
    "\n",
    "content_path = 'lab2_images/content.jpg'\n",
    "style_path = 'lab2_images/style.jpg'\n",
    "\n",
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "# only need to resize for signal SVD\n",
    "# style_image = resize(style_image,content_image.shape)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image,'Content')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image,'Style')\n",
    "\n",
    "tmp = {'style':style_image, \n",
    "       'content':content_image}\n",
    "\n",
    "alphas = {'layer3':0.8, 'layer2':0.6, 'layer1':0.6}\n",
    "decoded_images = AE(tmp, alphas=alphas)\n",
    "\n",
    "imshow(style_image,'Style')\n",
    "for layer in decoded_images.keys():\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    imshow(decoded_images[layer][0],'Styled')\n",
    "    plt.subplot(1,2,2)\n",
    "    imshow(decoded_images[layer][1],'Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-future",
   "metadata": {},
   "source": [
    "## WHITENING, COLORING, TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-fortune",
   "metadata": {},
   "source": [
    "### Whitening \n",
    "\n",
    "We remove the correlation in the signal by making the covariance almost the identity using Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wct(content, style, alpha=0.6, eps=1e-5):\n",
    "        '''\n",
    "        https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "           Perform Whiten-Color Transform on feature maps using numpy\n",
    "           See p.4 of the Universal Style Transfer paper for equations:\n",
    "           https://arxiv.org/pdf/1705.08086.pdf\n",
    "        '''\n",
    "        # We squeeze the input to 3-D and transpose them\n",
    "        # 1xHxWxC -> CxHxW\n",
    "        content_transpose = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "        style_transpose = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "        # Flatten each channel into rows\n",
    "        # CxHxW -> CxH*W\n",
    "        content_flat = content_transpose.reshape(-1, content_transpose.shape[1]*content_transpose.shape[2])\n",
    "        style_flat = style_transpose.reshape(-1, style_transpose.shape[1]*style_transpose.shape[2])\n",
    "\n",
    "        # Threshold for eigenvalues\n",
    "        eigen_val_thresh = 1e-5\n",
    "        \n",
    "        # ===Whitening transform===\n",
    "        \n",
    "        # We take the mean along each channel, while keeping each vector dimension. \n",
    "        mean_content = content_flat.mean(axis=1, keepdims=True)\n",
    "        # We take the covariance by taking the difference between the channel and its expected value (mean).\n",
    "        flat_content = content_flat - mc\n",
    "        \n",
    "        # Take the matricial dot product between the flat content and its transpose.\n",
    "        cov_content = np.dot(flat_content, flat_content.T) / (content_transpose.shape[1]*content_transpose.shape[2] - 1)\n",
    "        \n",
    "        # Take the singular value decomposition (SVD), Uc = U-content Sc= S-content \n",
    "        Uc, Sc, _ = np.linalg.svd(cov_c)\n",
    "        \n",
    "        # Use threshold to truncate the SVD to only the largest EV.\n",
    "        # Sum over the eigenvalues greater than the threshold, the vectors are arranged in descending order.\n",
    "        k_c = (Sc > eigen_val_thresh).sum()\n",
    "        \n",
    "        # Create a diagonal matrix with the eigenvalues found before\n",
    "        Dc = np.diag((Sc[:k_c]+eps)**-0.5) # Consistent with the paper on universal style transfer\n",
    "        \n",
    "        # Truncate the rectangular matrix as well.\n",
    "        Uc = Uc[:,:k_c]\n",
    "        \n",
    "        # Now make a whitened content image\n",
    "        fc_white = (Uc @ Dc @ Uc.T) @ flat_content\n",
    "\n",
    "        # ===Coloring transform===\n",
    "        \n",
    "        # Same process as before for coloring using style \n",
    "        mean_style = style_flat.mean(axis=1, keepdims=True)\n",
    "        flat_style = style_flat - ms\n",
    "        \n",
    "        # 2. get covariance of style, take SVD\n",
    "        cov_style = np.dot(flat_style, flat_style.T) / (style_transpose.shape[1]*style_transpose.shape[2] - 1)\n",
    "        # Us = U-style Ss= S-style \n",
    "        Us, Ss, _ = np.linalg.svd(cov_style)\n",
    "        \n",
    "        # 3. truncate the SVD to only the largest eigen values\n",
    "        k_s = (Ss > eigen_val_thresh).sum()\n",
    "        Ds = np.sqrt(np.diag(Ss[:k_s]+eps))\n",
    "        Us = Us[:,:k_s]\n",
    "        \n",
    "        # 4. Now make a colored image that mixes the Grammian of the style\n",
    "        #   with the whitened content image\n",
    "        fcs_hat = (Us @ Ds @ Us.T) @ fc_white # Consistent with the paper\n",
    "        fcs_hat = fcs_hat + mean_style # add style mean back to each channel\n",
    "\n",
    "        # Blend transform features with original features, apply alpha to change intensity of style over the content image\n",
    "        blended = alpha*fcs_hat + (1 - alpha)*(content_flat) \n",
    "\n",
    "        # Get back to the original shape\n",
    "        # CxH*W -> CxHxW\n",
    "        blended = blended.reshape(content_t.shape)\n",
    "        # CxHxW -> 1xHxWxC\n",
    "        blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "        return np.float32(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "headed-scott",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                  Type                  Data/Info\n",
      "---------------------------------------------------------\n",
      "BATCH_SIZE                int                   4\n",
      "CONTENT_TRAINING_SIZE     tuple                 n=3\n",
      "Callback                  type                  <class 'tensorflow.python<...>eras.callbacks.Callback'>\n",
      "Conv2D                    type                  <class 'tensorflow.python<...>rs.convolutional.Conv2D'>\n",
      "EncoderDecoder            type                  <class '__main__.EncoderDecoder'>\n",
      "H                         History               <tensorflow.python.keras.<...>ct at 0x000002635B388760>\n",
      "Image                     module                <module 'PIL.Image' from <...>packages\\\\PIL\\\\Image.py'>\n",
      "ImageDataGenerator        type                  <class 'tensorflow.python<...>mage.ImageDataGenerator'>\n",
      "Input                     function              <function Input at 0x000002632FA1C040>\n",
      "K                         module                <module 'tensorflow.keras<...>s\\\\backend\\\\__init__.py'>\n",
      "LAMBDA                    int                   1\n",
      "MEAN_PIXEL                ndarray               3: 3 elems, type `float64`, 24 bytes\n",
      "MaxPooling2D              type                  <class 'tensorflow.python<...>rs.pooling.MaxPooling2D'>\n",
      "Model                     type                  <class 'tensorflow.python<...>s.engine.training.Model'>\n",
      "Path                      type                  <class 'pathlib.Path'>\n",
      "PurePath                  type                  <class 'pathlib.PurePath'>\n",
      "Sequential                type                  <class 'tensorflow.python<...>e.sequential.Sequential'>\n",
      "TARGET_SIZE               tuple                 n=2\n",
      "TRAIN_PATH                str                   dogcat/dataset/training_set\n",
      "UpSampling2D              type                  <class 'tensorflow.python<...>volutional.UpSampling2D'>\n",
      "VAL_PATH                  str                   data/val\n",
      "VGG19                     function              <function VGG19 at 0x000002635AF090D0>\n",
      "WEIGHTS_PATH              str                   C:\\Users\\Alejandro de Leo<...>ering_tf_kernels_notop.h5\n",
      "WEIGHTS_PATH_NO_TOP       str                   https://github.com/fcholl<...>ering_tf_kernels_notop.h5\n",
      "applications              module                <module 'tensorflow.keras<...>plications\\\\__init__.py'>\n",
      "count_num_samples         function              <function count_num_sampl<...>es at 0x00000262E33DA040>\n",
      "create_gen                function              <function create_gen at 0x000002635AF01E50>\n",
      "datagen                   ImageDataGenerator    <tensorflow.python.keras.<...>ct at 0x000002635AF8C490>\n",
      "decoder_layers            function              <function decoder_layers at 0x00000262E3457550>\n",
      "disable_eager_execution   function              <function disable_eager_e<...>on at 0x000002632DF84280>\n",
      "enable_eager_execution    function              <function enable_eager_ex<...>on at 0x000002632DF841F0>\n",
      "encoder_decoder           EncoderDecoder        <__main__.EncoderDecoder <...>ct at 0x000002635AF8C430>\n",
      "epochs                    int                   2\n",
      "get_file                  function              <function get_file at 0x000002632FAD88B0>\n",
      "h5py                      module                <module 'h5py' from 'C:\\\\<...>ages\\\\h5py\\\\__init__.py'>\n",
      "image                     module                <module 'tensorflow.keras<...>ing\\\\image\\\\__init__.py'>\n",
      "keras                     module                <module 'tensorflow.keras<...>low\\\\keras\\\\__init__.py'>\n",
      "l2_loss                   function              <function l2_loss at 0x000002635AF09280>\n",
      "layers                    module                <module 'tensorflow.keras<...>as\\\\layers\\\\__init__.py'>\n",
      "load_model                function              <function load_model at 0x0000026357135670>\n",
      "load_weights              function              <function load_weights at 0x00000262E342EE50>\n",
      "losses                    module                <module 'tensorflow.keras<...>as\\\\losses\\\\__init__.py'>\n",
      "np                        module                <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "num_samples               int                   8000\n",
      "os                        module                <module 'os' from 'c:\\\\pr<...>s\\\\python38\\\\lib\\\\os.py'>\n",
      "pathlib                   module                <module 'pathlib' from 'c<...>thon38\\\\lib\\\\pathlib.py'>\n",
      "plt                       module                <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "preprocess_input          function              <function preprocess_input at 0x000002635AF09040>\n",
      "resize                    function              <function resize at 0x000002635A643C10>\n",
      "skimage                   module                <module 'skimage' from 'c<...>s\\\\skimage\\\\__init__.py'>\n",
      "steps_per_epoch           int                   2000\n",
      "sys                       module                <module 'sys' (built-in)>\n",
      "target_layer              int                   2\n",
      "tf                        module                <module 'tensorflow' from<...>tensorflow\\\\__init__.py'>\n",
      "train_gen                 generator             <generator object create_<...>en at 0x000002635AF8D900>\n",
      "vgg_layers                function              <function vgg_layers at 0x00000262E342E550>\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-twenty",
   "metadata": {},
   "source": [
    "References:\n",
    "- [1]\n",
    "- [2] https://www.dropbox.com/sh/2djb2c0ohxtvy2t/AAAxA2dnoFBcHGqfP0zLx-Oua?dl=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
