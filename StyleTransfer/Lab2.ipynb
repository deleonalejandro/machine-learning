{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revised-press",
   "metadata": {},
   "source": [
    "# Lab 1: CNN Visualization\n",
    "\n",
    "*Team Members:*\n",
    "- Yasmin Femerling\n",
    "- Alejandro de Leon\n",
    "\n",
    "March 21, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-finish",
   "metadata": {},
   "source": [
    "In this lab we implement a photo realistic style transfer algorithm using the work of Li et al. in their universal style transfer paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "animal-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, applications\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-minneapolis",
   "metadata": {},
   "source": [
    "- vgg tiene stride de 1\n",
    "- convolutional layers de 3x3\n",
    "- cambiamos el numero de las layers\n",
    "- cambiamos el size del conv2d del upsampling pq tiene que ser un factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "foreign-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "    if layer == 1:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "    x = Conv2D(512, (4, 4), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    if layer == 2:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "    x = Conv2D(256, (4, 4), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "    if layer == 3:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "    x = Conv2D(128, (4, 4), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    if layer == 4:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "    x = Conv2D(64, (4, 4), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    if layer == 5:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caring-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_num_samples(directory):\n",
    "    total=0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total\n",
    "\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        WEIGHTS_PATH_NO_TOP,\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    f = h5py.File(WEIGHTS_PATH)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            g = f[b_name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)       \n",
    "        \n",
    "    print(\"Before model\", inputs)\n",
    "    \n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer))\n",
    "    load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-tenant",
   "metadata": {},
   "source": [
    "- training decoder from early layer ('block2_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "thorough-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA=1\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "def create_loss_fn(y_true,y_pred):\n",
    "    out_pred = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(y_pred)\n",
    "    out_true = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(y_true)\n",
    "    loss = l2_loss(y_pred - y_true) + l2_loss(out_pred - out_true)\n",
    "        \n",
    "    return loss\n",
    "    \n",
    "## STYLE-TRANSFER LOSS\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        \n",
    "        print(\"Creando modelo...\")\n",
    "        self.encoder = VGG19(input_shape=self.input_shape, target_layer=target_layer)\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "        \n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "        \n",
    "        self.finalmodel = Model(self.model.inputs, [self.encoder, self.model])\n",
    "\n",
    "        self.model.compile('adam', create_loss_fn)\n",
    "        self.model.summary()\n",
    "        print(\"Termine modelo...\")\n",
    "        \n",
    "    def create_loss_fn(self):\n",
    "        def get_encodings(inputs):\n",
    "            self.encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-globe",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "remarkable-mambo",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 100000 images belonging to 200 classes.\n",
      "Creando modelo...\n",
      "Before model Tensor(\"input_1:0\", shape=(None, 256, 256, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-79ffc455b505>:55: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(WEIGHTS_PATH)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-5-9aa955534e49>:8 create_loss_fn  *\n        out_true = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(y_true)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:766 __call__  **\n        self._maybe_build(inputs)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:2106 _maybe_build\n        self.build(input_shapes)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:188 build\n        input_channel = self._get_input_channel(input_shape)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:360 _get_input_channel\n        raise ValueError('The channel dimension of the inputs '\n\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6c99f5ad65b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Initializing encoder-decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mencoder_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-471e258b617c>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, target_layer, decoder_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_loss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Termine modelo...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m       \u001b[1;31m# Creates the model loss and weighted metrics sub-graphs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_weights_loss_and_weighted_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[1;31m# Functions for train, test and predict will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_compile_weights_loss_and_weighted_metrics\u001b[1;34m(self, sample_weights)\u001b[0m\n\u001b[0;32m   1541\u001b[0m       \u001b[1;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m       \u001b[1;31m#                   layer losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1543\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m   1601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1602\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reduction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1603\u001b[1;33m             \u001b[0mper_sample_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1604\u001b[0m             weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[0;32m   1605\u001b[0m                 \u001b[0mper_sample_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    254\u001b[0m           y_pred, y_true)\n\u001b[0;32m    255\u001b[0m     \u001b[0mag_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-5-9aa955534e49>:8 create_loss_fn  *\n        out_true = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(y_true)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:766 __call__  **\n        self._maybe_build(inputs)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:2106 _maybe_build\n        self.build(input_shapes)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:188 build\n        input_channel = self._get_input_channel(input_shape)\n    c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:360 _get_input_channel\n        raise ValueError('The channel dimension of the inputs '\n\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = 'data/train'\n",
    "VAL_PATH = 'data/val'\n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 4\n",
    "epochs = 2\n",
    "target_layer = 2\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "gen = datagen.flow_from_directory(TRAIN_PATH, target_size=TARGET_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, class_mode=None)\n",
    "\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "\n",
    "# Creating generators\n",
    "train_gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "#validation_gen = create_gen(VAL_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "\n",
    "# Steps per epoc calculation\n",
    "num_samples = count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "\n",
    "# Validation steps calculation\n",
    "#num_samples = count_num_samples(VAL_PATH)\n",
    "#validation_steps = num_samples // BATCH_SIZE\n",
    "\n",
    "# Initializing encoder-decoder\n",
    "K.clear_session()\n",
    "encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
    "\n",
    "\n",
    "# Training model\n",
    "H = encoder_decoder.model.fit(train_gen, \n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs = epochs, \n",
    "                             )\n",
    "\n",
    "# Plot model training loss and validation loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss and Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot-%d.png\" % target_layer)\n",
    "\n",
    "# Saving decoder\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
