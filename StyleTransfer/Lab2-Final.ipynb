{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "retired-cornell",
   "metadata": {},
   "source": [
    "# Lab 2: Style Transfer\n",
    "*Team Members:*\n",
    "- Yasmin Femerling\n",
    "- Alejandro de Leon\n",
    "\n",
    "March, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-fault",
   "metadata": {},
   "source": [
    " In this lab we implement a photo realistic style transfer algorithm using the work of Li et al. in their universal style transfer paper. [1]\n",
    " \n",
    " ## Encoder Decoder Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprised-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, applications, losses\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import get_file\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib \n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.python.framework.ops import enable_eager_execution\n",
    "enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-syndicate",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will make use of a VGG encoder and a decoder to make the style transfer. Style transfer is seen as an image reconstruction task, so the encoder will extract features from an image to then apply a transformation that will allow for style transfer. Once the transformation is done, the VGG decoder can reconstruct the image from the transformed features. \n",
    "\n",
    "In the code below we construct a decoder based on the work of Justinledford [2]. The decoder is based on the VGG model that will be seen below. The VGG layers are composed of convolutional layers and MaxPooling. To make a decoder, the use of convolutional layers and UpSampling is needed, making the reverse process. \n",
    "\n",
    "A few corrections were made to the decoder. First, it is based on numbered layers. The VGG encoder-decoder can be made with different relu layers from the 5 blocks of the VGG. How far along the layers the decoder goes in the VGG encoder, is how far along on the layers the decoder has to go in inverse. The convolutional layers are normally 3x3, but in the layer right after an upsampling, we change it to 4x4 to have it be an integer of the upsampling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder makes the inverse from the encoder layers\n",
    "def decoder_layers(inputs, layer):\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    if layer >= 5: \n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(x)\n",
    "        x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "        x = Conv2D(512, (4, 4), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "        \n",
    "    if layer >= 4: \n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "        x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "        x = Conv2D(256, (4, 4), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    \n",
    "    \n",
    "    if layer >= 3:\n",
    "\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "        x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "        x = Conv2D(128, (4, 4), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "     \n",
    "    if layer >= 2: \n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "        x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "        x = Conv2D(64, (4, 4), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    \n",
    "    if layer >= 1:\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-dragon",
   "metadata": {},
   "source": [
    "Because the VGG encoder's job will be to extract features, this does not have to be trained. That is why the VGG layers will be constructed using the weights from fchollet's release. [3] We do not need the top layers from VGG because there will be no classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will count the number of samples in a directory (including its subdirectories)\n",
    "def count_num_samples(directory):\n",
    "    total=0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total\n",
    "\n",
    "# Weights from the trained VGG\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "# Get weights without top\n",
    "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        WEIGHTS_PATH_NO_TOP,\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "\n",
    "# Making the VGG until the desired relu layer. \n",
    "def vgg_layers(inputs, target_layer):\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# The layers were made to match the weights\n",
    "def load_weights(model):\n",
    "    f = h5py.File(WEIGHTS_PATH)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            g = f[b_name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL\n",
    "\n",
    "# Making the VGG function\n",
    "def VGG19(img_in=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if img_in is not None:\n",
    "        if len(K.int_shape(img_in)) == 3:\n",
    "            img_in = K.expand_dims(img_in, axis=0)\n",
    "        \n",
    "        img_in = preprocess_input(img_in)\n",
    "    \n",
    "    if img_in is None:\n",
    "        \n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=img_in, shape=input_shape)\n",
    "        \n",
    "    print(inputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name=\"vgg19\")\n",
    "    load_weights(model)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-dutch",
   "metadata": {},
   "source": [
    "We have the functions that will create an encoder and a decoder. To make a model from them, the encoder output must be the new input of the decoder. They will be joined using a sequential keras model and we will create a loss function that takes into account both the content of the image and the features of the image. That way, the model can be trained based on the substraction of the input image and the reconstructed image (how much the content is similar) and the substraction of the VGG features from the input image and the reconstructed image. The better the decoder can be trained, the better the style transfer can be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occupational-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA=1\n",
    "\n",
    "# L2 is used for the loss function \n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "    \n",
    "## STYLE-TRANSFER LOSS\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "# This class will hold the encoder and the decoder using the above functions\n",
    "class EncoderDecoder:\n",
    "    \n",
    "    # The encoder and decoder will be initialized based on the input_shape and the target layer \n",
    "    # that we specify\n",
    "    \n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        \n",
    "        # The encoder will have the desider input shape and go until the target layer\n",
    "        self.encoder = VGG19(input_shape=self.input_shape, target_layer=target_layer)\n",
    "        \n",
    "        # The decoder will be created based on the target layer\n",
    "        self.decoder = self.create_decoder(target_layer)\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "        \n",
    "         # We make a vgg that will be used in the loss function\n",
    "        self.vgg_loss = VGG19(input_shape=self.model.output_shape, target_layer=target_layer)\n",
    "        \n",
    "        # We create the loss function\n",
    "        self.loss = self.create_loss_fn()\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "        self.model.summary()\n",
    "        \n",
    "    def create_loss_fn(self):\n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            content_image = y_true\n",
    "            reconstructed_image = y_pred\n",
    "            \n",
    "            # We take into account the content and the features of the \n",
    "            # output \"reconstructed\"  image for the loss function\n",
    "            encoding_in = self.vgg_loss(content_image)\n",
    "            encoding_out = self.vgg_loss(reconstructed_image)\n",
    "            \n",
    "            # We take the l2 loss of both\n",
    "            return l2_loss(reconstructed_image - content_image) + \\\n",
    "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        \n",
    "        return custom_loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        \n",
    "        # We create the decoder by getting the output of the encoder as the new input\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        \n",
    "        # We add the final activation layer\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same', name='decoder_out')(layers)\n",
    "        decoder = Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "        \n",
    "        return decoder\n",
    "\n",
    "    # Function to save the encoderdecoder, with a name based on the target layer\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_night_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-track",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We will now train the model. The encoder is fixed, so only the decoder needs to be trained. We will make two models, one for target layer 2 and one for target layer 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'dogcat/dataset/train-chill'\n",
    "VAL_PATH = 'dogcat/dataset/test-chill' \n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "epochs = 4\n",
    "target_layer = 2\n",
    "\n",
    "# We use image data generator to have batches input for training\n",
    "# we preprocess the input for the VGG\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "\n",
    "# Creating generators\n",
    "# One is for training and the other for validation\n",
    "train_gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "validation_gen = create_gen(VAL_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "\n",
    "# Steps per epoc calculation\n",
    "num_samples = count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "\n",
    "# Validation steps calculation\n",
    "num_samples = count_num_samples(VAL_PATH)\n",
    "validation_steps = num_samples // BATCH_SIZE\n",
    "\n",
    "# Initializing encoder-decoder\n",
    "encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
    "\n",
    "\n",
    "# Training model\n",
    "# We use the fit function and specify the training\n",
    "# and validation data to have both\n",
    "H = encoder_decoder.model.fit(train_gen, \n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              validation_data=validation_gen,\n",
    "                              validation_steps=validation_steps,\n",
    "                              epochs = epochs, \n",
    "                             )\n",
    "\n",
    "# Saving decoder\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-layer",
   "metadata": {},
   "source": [
    "The EncoderDecoders were trained and the following results were obtained: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-swaziland",
   "metadata": {},
   "source": [
    "## Encoder-Decoder A (Layer 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "heavy-architecture",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "vgg19 (Functional)           (None, 128, 128, 128)     112576    \n",
    "_________________________________________________________________\n",
    "decoder_2 (Functional)       (None, 256, 256, 3)       11878403  \n",
    "=================================================================\n",
    "Total params: 11,990,979\n",
    "Trainable params: 11,878,403\n",
    "Non-trainable params: 112,576\n",
    "_________________________________________________________________\n",
    "Termine modelo...\n",
    "Epoch 1/4\n",
    "125/125 [==============================] - ETA: 0s - batch: 62.0000 - size: 8.0000 - loss: 140573331521667.0781 \n",
    "C:\\Users\\Alejandro de Leon\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
    "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
    "125/125 [==============================] - 7458s 60s/step - batch: 62.0000 - size: 8.0000 - loss: 140573331521667.0781 - val_loss: 286280386805.7600\n",
    "Epoch 2/4\n",
    "125/125 [==============================] - 7166s 57s/step - batch: 62.0000 - size: 8.0000 - loss: 282534414647.2960 - val_loss: 283699775078.4000\n",
    "Epoch 3/4\n",
    "125/125 [==============================] - 7312s 59s/step - batch: 62.0000 - size: 8.0000 - loss: 282534415958.0160 - val_loss: 297394058690.5600\n",
    "Epoch 4/4\n",
    "125/125 [==============================] - 7588s 61s/step - batch: 62.0000 - size: 8.0000 - loss: 282534418972.6720 - val_loss: 279942553600.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-penny",
   "metadata": {},
   "source": [
    "## Encoder-Decoder B (Layer 4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "understood-dominant",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "vgg19 (Functional)           (None, 32, 32, 512)       3505728   \n",
    "_________________________________________________________________\n",
    "decoder_4 (Functional)       (None, 256, 256, 3)       7069251   \n",
    "=================================================================\n",
    "Total params: 10,574,979\n",
    "Trainable params: 7,069,251\n",
    "Non-trainable params: 3,505,728\n",
    "_________________________________________________________________\n",
    "Termine modelo...\n",
    "Epoch 1/4\n",
    "125/125 [==============================] - ETA: 0s - batch: 62.0000 - size: 8.0000 - loss: 1485902582382.5920 \n",
    "C:\\Users\\Alejandro de Leon\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
    "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
    "125/125 [==============================] - 3008s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1485902582382.5920 - val_loss: 1436333167083.5200\n",
    "Epoch 2/4\n",
    "125/125 [==============================] - 2995s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1428112203055.1040 - val_loss: 1442234121584.6399\n",
    "Epoch 3/4\n",
    "125/125 [==============================] - 2998s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1428112221929.4719 - val_loss: 1419905225195.5200\n",
    "Epoch 4/4\n",
    "125/125 [==============================] - 2998s 24s/step - batch: 62.0000 - size: 8.0000 - loss: 1428112230842.3682 - val_loss: 1421710665973.7600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-union",
   "metadata": {},
   "source": [
    "## Encoder-Decoder C (Layer 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "restricted-proposal",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "vgg19 (Functional)           (None, 128, 128, 128)     112576    \n",
    "_________________________________________________________________\n",
    "decoder_2 (Functional)       (None, 256, 256, 3)       11878403  \n",
    "=================================================================\n",
    "Total params: 11,990,979\n",
    "Trainable params: 11,878,403\n",
    "Non-trainable params: 112,576\n",
    "_________________________________________________________________\n",
    "Termine modelo...\n",
    "Epoch 1/2\n",
    "2000/2000 [==============================] - 46536s 23s/step - batch: 999.5000 - size: 4.0000 - loss: 669974751655.9360\n",
    "Epoch 2/2\n",
    "2000/2000 [==============================] - 52873s 26s/step - batch: 999.5000 - size: 4.0000 - loss: 126673469906.9440"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-twins",
   "metadata": {},
   "source": [
    "The results were not good. Below are the plots of the models from layer 2 (A) and layer 4 (B): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model training loss and validation loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot-%d.png\" % target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-tuesday",
   "metadata": {},
   "source": [
    "## Evaluate decoder\n",
    "We now test and evaluate the encoder-decoder for an image reconstruction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODER_PATH = 'decoder_4.h5'\n",
    "INPUT_IMG_PATH = 'dogcat/dataset/train-chill/cats/cat.3.jpg'\n",
    "OUTPUT_IMG_PATH = 'dog_recon.jpg'\n",
    "TARGET_LAYER = 4\n",
    "\n",
    "encoder_decoder = EncoderDecoder(decoder_path=DECODER_PATH, target_layer=TARGET_LAYER)\n",
    "\n",
    "# We load the test image from the input path\n",
    "input_img = image.load_img(INPUT_IMG_PATH, target_size = (256, 256))\n",
    "\n",
    "# Transform the image into an array\n",
    "input_img = image.img_to_array(input_img)\n",
    "\n",
    "# Preprocess\n",
    "input_img = preprocess_input(input_img)\n",
    "\n",
    "# Expand dimensions to match the input of the model 'batches'\n",
    "input_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "# Use the model to predict\n",
    "output_img = encoder_decoder.model.predict([input_img])[0]\n",
    "\n",
    "output_img /= np.amax(output_img)\n",
    "\n",
    "# Save the image\n",
    "plt.imsave(OUTPUT_IMG_PATH, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-banking",
   "metadata": {},
   "source": [
    "Because the training of our encoder-decoder model did not yield such good results, we will be using a pretrained autoencoder to show images and their reconstructions. The autoencoder was taken from Yihao Wang's code. [4] The autoencoder shown below is taken from his work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-twenty",
   "metadata": {},
   "source": [
    "References:\n",
    "- [1] Li, Yijun, et al. \"Universal style transfer via feature transforms.\" arXiv preprint arXiv:1705.08086 (2017).\n",
    "- [2] https://github.com/8000net/universal-style-transfer-keras\n",
    "- [3] https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "- [4] https://www.dropbox.com/sh/2djb2c0ohxtvy2t/AAAxA2dnoFBcHGqfP0zLx-Oua?dl=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
