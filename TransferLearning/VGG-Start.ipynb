{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "valid-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from ipynb.fs.full.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "earned-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokemonNet:\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_type1_branch(inputs, numType1, finalAct=\"sigmoid\"):\n",
    "        \n",
    "        x = Flatten()(inputs)\n",
    "        # add two fully connected layers and some dropout\n",
    "        x = Dense(256)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(numType1)(x)\n",
    "        x = Activation(finalAct, name=\"type1_classification\")(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_type2_branch(inputs, numType2, finalAct=\"sigmoid\"):\n",
    "        \n",
    "        x = Flatten()(inputs)\n",
    "        # add two fully connected layers and some dropout\n",
    "        x = Dense(256)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(numType2)(x)\n",
    "        x = Activation(finalAct, name=\"type2_classification\")(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_model(width, height, numType1, numType2, finalAct=\"sigmoid\"):\n",
    "        \n",
    "        inputs = Input(shape=(width, height, 3))\n",
    "        \n",
    "        print(\"[Building VGG16...]\")\n",
    "        vgg_model = VGG16(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            input_tensor=inputs,\n",
    "            input_shape=(width, height),\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        # CONV => RELU => POOL\n",
    "        x = Conv2D(32, (3, 3), padding=\"same\")(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        x = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Constructing type 1 and type 2 heads.\n",
    "        type1_branch = PokemonNet.build_type1_branch(vgg_model.output, numType1, finalAct)\n",
    "        type2_branch = PokemonNet.build_type2_branch(vgg_model.output, numType2, finalAct)\n",
    "        \n",
    "        # Building the model\n",
    "        print(\"Building model...\")\n",
    "        model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs=[type1_branch, type2_branch],\n",
    "            name=\"pokemonnet\"\n",
    "                     )\n",
    "        \n",
    "        return model    \n",
    "    \n",
    "    @staticmethod\n",
    "    def net_generator(generator, data, label1, label2, batch_s):\n",
    "        gen1 = generator.flow(data, label1, batch_size=batch_s)\n",
    "        gen2 = generator.flow(data, label2, batch_size=batch_s)\n",
    "        while True:\n",
    "            X1 = gen1.next()\n",
    "            X2 = gen2.next()\n",
    "            yield X1[0], [X1[1], X2[1]] \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "confirmed-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model: \"pokemonnet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 150, 150, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 150, 150, 32) 896         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 150, 150, 32) 0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 50, 50, 32)   0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 50, 50, 32)   0           max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 50, 50, 64)   18496       dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 50, 50, 64)   0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 50, 50, 64)   36928       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 50, 50, 64)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 25, 25, 64)   0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 25, 25, 64)   0           max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 25, 25, 128)  73856       dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 25, 25, 128)  0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 25, 25, 128)  147584      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 25, 25, 128)  0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 12, 12, 128)  0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 12, 12, 128)  0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 18432)        0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18432)        0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 256)          4718848     flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 256)          4718848     flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 256)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 256)          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 256)          0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 256)          0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 18)           4626        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 19)           4883        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "type1_classification (Activatio (None, 18)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "type2_classification (Activatio (None, 19)           0           dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,724,965\n",
      "Trainable params: 9,724,965\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro de Leon\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Alejandro de Leon\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 11s 506ms/step - loss: 5.3786 - type1_classification_loss: 2.8727 - type2_classification_loss: 2.5059 - type1_classification_accuracy: 0.1055 - type2_classification_accuracy: 0.3739 - val_loss: 4.6943 - val_type1_classification_loss: 2.7494 - val_type2_classification_loss: 1.9449 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 10s 503ms/step - loss: 4.7348 - type1_classification_loss: 2.6657 - type2_classification_loss: 2.0691 - type1_classification_accuracy: 0.1722 - type2_classification_accuracy: 0.4957 - val_loss: 4.7741 - val_type1_classification_loss: 2.7500 - val_type2_classification_loss: 2.0241 - val_type1_classification_accuracy: 0.1605 - val_type2_classification_accuracy: 0.5247\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 10s 504ms/step - loss: 4.8280 - type1_classification_loss: 2.7099 - type2_classification_loss: 2.1181 - type1_classification_accuracy: 0.1493 - type2_classification_accuracy: 0.4809 - val_loss: 4.7169 - val_type1_classification_loss: 2.7350 - val_type2_classification_loss: 1.9819 - val_type1_classification_accuracy: 0.1173 - val_type2_classification_accuracy: 0.5185\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 10s 504ms/step - loss: 4.8899 - type1_classification_loss: 2.7400 - type2_classification_loss: 2.1499 - type1_classification_accuracy: 0.1356 - type2_classification_accuracy: 0.4968 - val_loss: 4.6026 - val_type1_classification_loss: 2.7255 - val_type2_classification_loss: 1.8771 - val_type1_classification_accuracy: 0.1543 - val_type2_classification_accuracy: 0.5864\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 10s 504ms/step - loss: 4.7401 - type1_classification_loss: 2.6950 - type2_classification_loss: 2.0451 - type1_classification_accuracy: 0.1503 - type2_classification_accuracy: 0.5037 - val_loss: 4.8165 - val_type1_classification_loss: 2.7961 - val_type2_classification_loss: 2.0204 - val_type1_classification_accuracy: 0.0988 - val_type2_classification_accuracy: 0.4938\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 4.8815 - type1_classification_loss: 2.7268 - type2_classification_loss: 2.1547 - type1_classification_accuracy: 0.1248 - type2_classification_accuracy: 0.4879 - val_loss: 4.6603 - val_type1_classification_loss: 2.7359 - val_type2_classification_loss: 1.9244 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5494\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 10s 488ms/step - loss: 4.7892 - type1_classification_loss: 2.6968 - type2_classification_loss: 2.0924 - type1_classification_accuracy: 0.1219 - type2_classification_accuracy: 0.4807 - val_loss: 4.6600 - val_type1_classification_loss: 2.7479 - val_type2_classification_loss: 1.9120 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 10s 499ms/step - loss: 4.8929 - type1_classification_loss: 2.7371 - type2_classification_loss: 2.1557 - type1_classification_accuracy: 0.1401 - type2_classification_accuracy: 0.4598 - val_loss: 4.7346 - val_type1_classification_loss: 2.7949 - val_type2_classification_loss: 1.9397 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5185\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 10s 501ms/step - loss: 4.8492 - type1_classification_loss: 2.6935 - type2_classification_loss: 2.1557 - type1_classification_accuracy: 0.1325 - type2_classification_accuracy: 0.4668 - val_loss: 4.6711 - val_type1_classification_loss: 2.7785 - val_type2_classification_loss: 1.8926 - val_type1_classification_accuracy: 0.1173 - val_type2_classification_accuracy: 0.5617\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 10s 498ms/step - loss: 4.8495 - type1_classification_loss: 2.7430 - type2_classification_loss: 2.1065 - type1_classification_accuracy: 0.1269 - type2_classification_accuracy: 0.4868 - val_loss: 4.5568 - val_type1_classification_loss: 2.7455 - val_type2_classification_loss: 1.8113 - val_type1_classification_accuracy: 0.1173 - val_type2_classification_accuracy: 0.5802\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 10s 504ms/step - loss: 4.8623 - type1_classification_loss: 2.7290 - type2_classification_loss: 2.1333 - type1_classification_accuracy: 0.1408 - type2_classification_accuracy: 0.4802 - val_loss: 4.6656 - val_type1_classification_loss: 2.7578 - val_type2_classification_loss: 1.9078 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5679\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 4.8193 - type1_classification_loss: 2.6803 - type2_classification_loss: 2.1391 - type1_classification_accuracy: 0.1443 - type2_classification_accuracy: 0.4761 - val_loss: 4.6816 - val_type1_classification_loss: 2.7417 - val_type2_classification_loss: 1.9400 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 4.7714 - type1_classification_loss: 2.7071 - type2_classification_loss: 2.0643 - type1_classification_accuracy: 0.1456 - type2_classification_accuracy: 0.4820 - val_loss: 4.6608 - val_type1_classification_loss: 2.7374 - val_type2_classification_loss: 1.9234 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 10s 498ms/step - loss: 4.7254 - type1_classification_loss: 2.6833 - type2_classification_loss: 2.0421 - type1_classification_accuracy: 0.1216 - type2_classification_accuracy: 0.4826 - val_loss: 4.6752 - val_type1_classification_loss: 2.7746 - val_type2_classification_loss: 1.9005 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5556\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 4.6541 - type1_classification_loss: 2.6843 - type2_classification_loss: 1.9698 - type1_classification_accuracy: 0.1518 - type2_classification_accuracy: 0.5231 - val_loss: 4.7423 - val_type1_classification_loss: 2.7540 - val_type2_classification_loss: 1.9882 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5123\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 10s 501ms/step - loss: 4.7281 - type1_classification_loss: 2.6845 - type2_classification_loss: 2.0436 - type1_classification_accuracy: 0.1212 - type2_classification_accuracy: 0.4978 - val_loss: 4.5599 - val_type1_classification_loss: 2.6936 - val_type2_classification_loss: 1.8663 - val_type1_classification_accuracy: 0.0988 - val_type2_classification_accuracy: 0.5741\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 10s 503ms/step - loss: 4.7580 - type1_classification_loss: 2.6863 - type2_classification_loss: 2.0717 - type1_classification_accuracy: 0.1390 - type2_classification_accuracy: 0.4799 - val_loss: 4.7240 - val_type1_classification_loss: 2.7735 - val_type2_classification_loss: 1.9504 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5309\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 10s 491ms/step - loss: 4.7218 - type1_classification_loss: 2.7041 - type2_classification_loss: 2.0177 - type1_classification_accuracy: 0.1359 - type2_classification_accuracy: 0.5084 - val_loss: 4.6610 - val_type1_classification_loss: 2.7411 - val_type2_classification_loss: 1.9199 - val_type1_classification_accuracy: 0.1358 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 4.7668 - type1_classification_loss: 2.7047 - type2_classification_loss: 2.0621 - type1_classification_accuracy: 0.1313 - type2_classification_accuracy: 0.4822 - val_loss: 4.6564 - val_type1_classification_loss: 2.7425 - val_type2_classification_loss: 1.9139 - val_type1_classification_accuracy: 0.1358 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 10s 491ms/step - loss: 4.6459 - type1_classification_loss: 2.6753 - type2_classification_loss: 1.9707 - type1_classification_accuracy: 0.1474 - type2_classification_accuracy: 0.5333 - val_loss: 4.6565 - val_type1_classification_loss: 2.7604 - val_type2_classification_loss: 1.8961 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 4.7385 - type1_classification_loss: 2.7105 - type2_classification_loss: 2.0280 - type1_classification_accuracy: 0.1450 - type2_classification_accuracy: 0.4966 - val_loss: 4.7245 - val_type1_classification_loss: 2.7925 - val_type2_classification_loss: 1.9320 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5494\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 4.8800 - type1_classification_loss: 2.6959 - type2_classification_loss: 2.1842 - type1_classification_accuracy: 0.1472 - type2_classification_accuracy: 0.4455 - val_loss: 4.6757 - val_type1_classification_loss: 2.7177 - val_type2_classification_loss: 1.9579 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5247\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 10s 505ms/step - loss: 4.7422 - type1_classification_loss: 2.7208 - type2_classification_loss: 2.0213 - type1_classification_accuracy: 0.1332 - type2_classification_accuracy: 0.5083 - val_loss: 4.6285 - val_type1_classification_loss: 2.7393 - val_type2_classification_loss: 1.8892 - val_type1_classification_accuracy: 0.1667 - val_type2_classification_accuracy: 0.5556\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 11s 534ms/step - loss: 4.8166 - type1_classification_loss: 2.7348 - type2_classification_loss: 2.0818 - type1_classification_accuracy: 0.1268 - type2_classification_accuracy: 0.4906 - val_loss: 4.6784 - val_type1_classification_loss: 2.7551 - val_type2_classification_loss: 1.9232 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 10s 513ms/step - loss: 4.7404 - type1_classification_loss: 2.7003 - type2_classification_loss: 2.0401 - type1_classification_accuracy: 0.1413 - type2_classification_accuracy: 0.5042 - val_loss: 4.6792 - val_type1_classification_loss: 2.7444 - val_type2_classification_loss: 1.9347 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 10s 501ms/step - loss: 4.7351 - type1_classification_loss: 2.6795 - type2_classification_loss: 2.0556 - type1_classification_accuracy: 0.1464 - type2_classification_accuracy: 0.4944 - val_loss: 4.6851 - val_type1_classification_loss: 2.7770 - val_type2_classification_loss: 1.9080 - val_type1_classification_accuracy: 0.1173 - val_type2_classification_accuracy: 0.5741\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 10s 506ms/step - loss: 4.8128 - type1_classification_loss: 2.7167 - type2_classification_loss: 2.0962 - type1_classification_accuracy: 0.1448 - type2_classification_accuracy: 0.4760 - val_loss: 4.6167 - val_type1_classification_loss: 2.6970 - val_type2_classification_loss: 1.9197 - val_type1_classification_accuracy: 0.1420 - val_type2_classification_accuracy: 0.5494\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 4.8129 - type1_classification_loss: 2.6893 - type2_classification_loss: 2.1236 - type1_classification_accuracy: 0.1625 - type2_classification_accuracy: 0.4757 - val_loss: 4.6605 - val_type1_classification_loss: 2.6781 - val_type2_classification_loss: 1.9824 - val_type1_classification_accuracy: 0.1420 - val_type2_classification_accuracy: 0.5247\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 4.7423 - type1_classification_loss: 2.7201 - type2_classification_loss: 2.0222 - type1_classification_accuracy: 0.1361 - type2_classification_accuracy: 0.4970 - val_loss: 4.7057 - val_type1_classification_loss: 2.7819 - val_type2_classification_loss: 1.9238 - val_type1_classification_accuracy: 0.1049 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 10s 482ms/step - loss: 4.7328 - type1_classification_loss: 2.6492 - type2_classification_loss: 2.0835 - type1_classification_accuracy: 0.1598 - type2_classification_accuracy: 0.4721 - val_loss: 4.6631 - val_type1_classification_loss: 2.7412 - val_type2_classification_loss: 1.9219 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5494\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 10s 489ms/step - loss: 4.8208 - type1_classification_loss: 2.7186 - type2_classification_loss: 2.1022 - type1_classification_accuracy: 0.1148 - type2_classification_accuracy: 0.4929 - val_loss: 4.6577 - val_type1_classification_loss: 2.7437 - val_type2_classification_loss: 1.9140 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 10s 497ms/step - loss: 4.7557 - type1_classification_loss: 2.6833 - type2_classification_loss: 2.0724 - type1_classification_accuracy: 0.1344 - type2_classification_accuracy: 0.4837 - val_loss: 4.7403 - val_type1_classification_loss: 2.7274 - val_type2_classification_loss: 2.0129 - val_type1_classification_accuracy: 0.1420 - val_type2_classification_accuracy: 0.5185\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 4.7255 - type1_classification_loss: 2.6956 - type2_classification_loss: 2.0299 - type1_classification_accuracy: 0.1596 - type2_classification_accuracy: 0.4937 - val_loss: 4.6450 - val_type1_classification_loss: 2.7686 - val_type2_classification_loss: 1.8764 - val_type1_classification_accuracy: 0.1049 - val_type2_classification_accuracy: 0.5926\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 10s 491ms/step - loss: 4.8132 - type1_classification_loss: 2.7286 - type2_classification_loss: 2.0847 - type1_classification_accuracy: 0.1278 - type2_classification_accuracy: 0.4899 - val_loss: 4.5819 - val_type1_classification_loss: 2.7265 - val_type2_classification_loss: 1.8555 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5741\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 10s 499ms/step - loss: 4.7183 - type1_classification_loss: 2.6797 - type2_classification_loss: 2.0386 - type1_classification_accuracy: 0.1145 - type2_classification_accuracy: 0.4872 - val_loss: 4.7128 - val_type1_classification_loss: 2.7370 - val_type2_classification_loss: 1.9757 - val_type1_classification_accuracy: 0.1358 - val_type2_classification_accuracy: 0.5123\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 10s 479ms/step - loss: 4.7882 - type1_classification_loss: 2.7091 - type2_classification_loss: 2.0791 - type1_classification_accuracy: 0.1224 - type2_classification_accuracy: 0.4803 - val_loss: 4.6572 - val_type1_classification_loss: 2.7335 - val_type2_classification_loss: 1.9238 - val_type1_classification_accuracy: 0.1358 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 10s 494ms/step - loss: 4.7317 - type1_classification_loss: 2.7178 - type2_classification_loss: 2.0139 - type1_classification_accuracy: 0.1089 - type2_classification_accuracy: 0.4973 - val_loss: 4.6675 - val_type1_classification_loss: 2.7395 - val_type2_classification_loss: 1.9280 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 10s 491ms/step - loss: 4.7543 - type1_classification_loss: 2.7041 - type2_classification_loss: 2.0502 - type1_classification_accuracy: 0.1267 - type2_classification_accuracy: 0.4842 - val_loss: 4.7166 - val_type1_classification_loss: 2.7421 - val_type2_classification_loss: 1.9744 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5247\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 4.6607 - type1_classification_loss: 2.7063 - type2_classification_loss: 1.9544 - type1_classification_accuracy: 0.1311 - type2_classification_accuracy: 0.5177 - val_loss: 4.7024 - val_type1_classification_loss: 2.8009 - val_type2_classification_loss: 1.9015 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5617\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 10s 506ms/step - loss: 4.7534 - type1_classification_loss: 2.6627 - type2_classification_loss: 2.0908 - type1_classification_accuracy: 0.1589 - type2_classification_accuracy: 0.4685 - val_loss: 4.5569 - val_type1_classification_loss: 2.6948 - val_type2_classification_loss: 1.8622 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 10s 487ms/step - loss: 4.7429 - type1_classification_loss: 2.6990 - type2_classification_loss: 2.0439 - type1_classification_accuracy: 0.1517 - type2_classification_accuracy: 0.4954 - val_loss: 4.7259 - val_type1_classification_loss: 2.7343 - val_type2_classification_loss: 1.9916 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5185\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 10s 487ms/step - loss: 4.7558 - type1_classification_loss: 2.7221 - type2_classification_loss: 2.0336 - type1_classification_accuracy: 0.1564 - type2_classification_accuracy: 0.5025 - val_loss: 4.6431 - val_type1_classification_loss: 2.7405 - val_type2_classification_loss: 1.9026 - val_type1_classification_accuracy: 0.1358 - val_type2_classification_accuracy: 0.5494\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 4.8114 - type1_classification_loss: 2.7073 - type2_classification_loss: 2.1041 - type1_classification_accuracy: 0.1281 - type2_classification_accuracy: 0.4794 - val_loss: 4.6610 - val_type1_classification_loss: 2.7477 - val_type2_classification_loss: 1.9133 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 10s 488ms/step - loss: 4.7802 - type1_classification_loss: 2.7277 - type2_classification_loss: 2.0525 - type1_classification_accuracy: 0.1308 - type2_classification_accuracy: 0.4904 - val_loss: 4.7325 - val_type1_classification_loss: 2.7478 - val_type2_classification_loss: 1.9847 - val_type1_classification_accuracy: 0.1049 - val_type2_classification_accuracy: 0.5123\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 10s 508ms/step - loss: 4.7693 - type1_classification_loss: 2.6893 - type2_classification_loss: 2.0800 - type1_classification_accuracy: 0.1253 - type2_classification_accuracy: 0.4864 - val_loss: 4.6266 - val_type1_classification_loss: 2.7179 - val_type2_classification_loss: 1.9087 - val_type1_classification_accuracy: 0.1173 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 10s 498ms/step - loss: 4.7884 - type1_classification_loss: 2.6740 - type2_classification_loss: 2.1145 - type1_classification_accuracy: 0.1500 - type2_classification_accuracy: 0.4774 - val_loss: 4.7748 - val_type1_classification_loss: 2.7938 - val_type2_classification_loss: 1.9811 - val_type1_classification_accuracy: 0.1235 - val_type2_classification_accuracy: 0.5185\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 4.7202 - type1_classification_loss: 2.6774 - type2_classification_loss: 2.0428 - type1_classification_accuracy: 0.1375 - type2_classification_accuracy: 0.4874 - val_loss: 4.7097 - val_type1_classification_loss: 2.7950 - val_type2_classification_loss: 1.9148 - val_type1_classification_accuracy: 0.1111 - val_type2_classification_accuracy: 0.5494\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 10s 477ms/step - loss: 4.8768 - type1_classification_loss: 2.7042 - type2_classification_loss: 2.1726 - type1_classification_accuracy: 0.1553 - type2_classification_accuracy: 0.4617 - val_loss: 4.6552 - val_type1_classification_loss: 2.7448 - val_type2_classification_loss: 1.9104 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 10s 491ms/step - loss: 4.7399 - type1_classification_loss: 2.6791 - type2_classification_loss: 2.0607 - type1_classification_accuracy: 0.1442 - type2_classification_accuracy: 0.4948 - val_loss: 4.6817 - val_type1_classification_loss: 2.7409 - val_type2_classification_loss: 1.9408 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5432\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 4.7650 - type1_classification_loss: 2.6927 - type2_classification_loss: 2.0723 - type1_classification_accuracy: 0.1473 - type2_classification_accuracy: 0.4778 - val_loss: 4.6922 - val_type1_classification_loss: 2.7395 - val_type2_classification_loss: 1.9527 - val_type1_classification_accuracy: 0.1296 - val_type2_classification_accuracy: 0.5309\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 28\n",
    "INIT_LR = 1e-3\n",
    "IMG_DIMS = [150, 150]\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "\n",
    "# Extracting images\n",
    "data = load_images('images\\images', IMG_DIMS)\n",
    "\n",
    "# Values between [0, 1]\n",
    "data = np.array(data, dtype='float')/255.0\n",
    "#plt.imshow(data[0,:,:,:])\n",
    "\n",
    "# Binarize sets of labels\n",
    "type1LB = LabelBinarizer()\n",
    "type2LB = LabelBinarizer()\n",
    "type1Labels = type1LB.fit_transform(getTypeLabels('pokemon_2.csv', 'Type1'))\n",
    "type2Labels = type2LB.fit_transform(getTypeLabels('pokemon_2.csv', 'Type2'))\n",
    "\n",
    "#ptypeLabels = label_dataset('Bug', 'pokemon_2.csv')\n",
    "#print(ptypeLabels)\n",
    "\n",
    "# Split data into training and validation\n",
    "split = train_test_split(data, type1Labels, type2Labels, test_size=0.2)\n",
    "(trainX, testX, trainType1Y, testType1Y, trainType2Y, testType2Y) = split\n",
    "\n",
    "# defining dictionary with respective losses.\n",
    "losses = {\n",
    "    \"type1_classification\": \"categorical_crossentropy\",\n",
    "    \"type2_classification\": \"categorical_crossentropy\",\n",
    "}\n",
    "\n",
    "# Creating model\n",
    "model = PokemonNet.build_model(IMG_DIMS[0], IMG_DIMS[1], \n",
    "                               numType1=len(type1LB.classes_),\n",
    "                               numType2=len(type2LB.classes_),\n",
    "                               finalAct= ACTIVATION)\n",
    "\n",
    "model.compile(optimizer='adam', loss=losses, metrics=[\"accuracy\"])\n",
    "\n",
    "# Displaying model summary\n",
    "model.summary()\n",
    "\n",
    "# Create an ImageDataGenerator and do Image Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "\"\"\"\n",
    "# train the network to perform multi-output classification\n",
    "H = model.fit(x=trainX, \n",
    "        y={\"type1_classification\": trainType1Y, \"type2_classification\": trainType2Y},\n",
    "        validation_data=(testX, \n",
    "            {\"type1_classification\": testType1Y, \"type2_classification\": testType2Y}),\n",
    "        steps_per_epoch=len(trainX)/BATCH_SIZE,\n",
    "        validation_steps=len(testX)/BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1)\n",
    "\"\"\"\n",
    "\n",
    "# train the network to perform multi-output classification\n",
    "H = model.fit(PokemonNet.net_generator(datagen, trainX, trainType1Y, trainType2Y, BATCH_SIZE),\n",
    "        validation_data= PokemonNet.net_generator(datagen, testX, testType1Y, testType2Y, BATCH_SIZE),\n",
    "        steps_per_epoch=len(trainX)/BATCH_SIZE,\n",
    "        validation_steps=len(testX)/BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "higher-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building VGG16...\n",
      "Found 809 images belonging to 1 classes.\n",
      "Saving bottleneck features (train)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\PIL\\Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 809 images belonging to 1 classes.\n",
      "Saving bottleneck features (validation)...\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = 'images'\n",
    "validation_data_dir = 'images'\n",
    "nb_train_samples = 809\n",
    "nb_validation_samples = 809\n",
    "epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#test_dir = '../data/test'\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Save bottleneck features from VGG\n",
    "#\n",
    "\n",
    "# build the VGG16 network, leaving off the top classifier layer\n",
    "# so we just get the features as output\n",
    "print('Building VGG16...')\n",
    "input_tensor = Input(shape=(img_width, img_height, 3))\n",
    "base_model = VGG16(weights='imagenet', include_top=False,\n",
    "        input_tensor=input_tensor)\n",
    "\n",
    "# Save training features\n",
    "datagen = ImageDataGenerator(rescale=1/255)\n",
    "generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None, # class mode set to None here, because images are loaded in order\n",
    "    shuffle=False)\n",
    "\n",
    "print('Saving bottleneck features (train)...')\n",
    "bottleneck_features_train = base_model.predict(\n",
    "    generator, nb_train_samples // batch_size)\n",
    "np.save('features/train.npy', bottleneck_features_train)\n",
    "\n",
    "# Save validation features\n",
    "generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "print('Saving bottleneck features (validation)...')\n",
    "bottleneck_features_validation = base_model.predict(\n",
    "    generator, nb_validation_samples // batch_size)\n",
    "\n",
    "np.save('features/validation.npy',\n",
    "        bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "driven-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHeadLayers(inputs):\n",
    "    # setup params and where to save features\n",
    "    \n",
    "    # flatten the output convolutions, some implementations also \n",
    "    #. perform an average pooling here to collapse the features down\n",
    "    x = Flatten()(inputs)\n",
    "    # add two fully connected layers and some dropout\n",
    "    x = Dense(256)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid', name=\"type_classification\")(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "suitable-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMultiTaskModel(inputShape, tasks):\n",
    "    \n",
    "    # construct both heads\n",
    "    inputs = Input(shape=inputShape)\n",
    "    output_grass = createHeadLayers(inputs)\n",
    "    \n",
    "    model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs= [output_grass],\n",
    "            name=\"pokemon\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "turkish-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 809 lines.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Epoch 1/30\n",
      "41/41 [==============================] - 1s 6ms/step - loss: 0.8905 - accuracy: 0.8842\n",
      "Epoch 2/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.3018 - accuracy: 0.9112\n",
      "Epoch 3/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.2675 - accuracy: 0.9142\n",
      "Epoch 4/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.2735 - accuracy: 0.9038\n",
      "Epoch 5/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1974 - accuracy: 0.9289\n",
      "Epoch 6/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1944 - accuracy: 0.9225\n",
      "Epoch 7/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1866 - accuracy: 0.9172\n",
      "Epoch 8/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.9255\n",
      "Epoch 9/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1651 - accuracy: 0.9331\n",
      "Epoch 10/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1252 - accuracy: 0.9505\n",
      "Epoch 11/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9431\n",
      "Epoch 12/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1083 - accuracy: 0.9507\n",
      "Epoch 13/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1227 - accuracy: 0.9426\n",
      "Epoch 14/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9528\n",
      "Epoch 15/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.9497\n",
      "Epoch 16/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0979 - accuracy: 0.9527\n",
      "Epoch 17/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9696\n",
      "Epoch 18/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9774\n",
      "Epoch 19/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9729\n",
      "Epoch 20/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9799\n",
      "Epoch 21/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0650 - accuracy: 0.9750\n",
      "Epoch 22/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 0.9848\n",
      "Epoch 23/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9825\n",
      "Epoch 24/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 0.9786\n",
      "Epoch 25/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0459 - accuracy: 0.9867\n",
      "Epoch 26/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0434 - accuracy: 0.9823\n",
      "Epoch 27/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.9830\n",
      "Epoch 28/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0573 - accuracy: 0.9703\n",
      "Epoch 29/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9820: 0s - loss: 0.0551 - accuracy: \n",
      "Epoch 30/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "INIT_LR = 1e-3\n",
    "BS = 20\n",
    "\n",
    "# Load bottleneck features training\n",
    "train_data = np.load('features/train.npy')\n",
    "\n",
    "# Load bottleneck features validation\n",
    "validation_data = np.load('features/validation.npy')\n",
    "\n",
    "tasks = ['Poison']\n",
    "\n",
    "train_labels = label_dataset(tasks[0], 'pokemon_2.csv')\n",
    "print(train_labels)\n",
    "inputs = Input(shape=input_shape[1:])\n",
    "output = createHeadLayers(inputs)\n",
    "\n",
    "data = load_images('images\\images', [150, 150])\n",
    "data = np.array(data, dtype='float')/255.0\n",
    "\n",
    "# Extracting labels\n",
    "ptypeLabels = label_dataset('Grass', 'pokemon_2.csv')\n",
    "\n",
    "# binarize labels\n",
    "\n",
    "\n",
    "split = train_test_split(data, ptypeLabels, test_size=0.2)\n",
    "(trainX, testX, trainTypeY, testTypeY) = split\n",
    "\n",
    "\n",
    "model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs= output,\n",
    "            name=\"pokemon\")\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_data, train_labels,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BS,\n",
    "              verbose=1)\n",
    "\n",
    "model.save('pokemon_model.h5', save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aggressive-clarity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=150x150 at 0x1DF94F086D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DF94F9A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img = image.load_img('images/images/venusaur.png', target_size = (150, 150))\n",
    "display(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "model = load_model('pokemon_model.h5')\n",
    "\n",
    "x = base_model.predict(img)\n",
    "result = model.predict(x)\n",
    "\n",
    "print(result)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acting-europe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b2593a47bcca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mplot_training_validation_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-b2593a47bcca>\u001b[0m in \u001b[0;36mplot_training_validation_acc\u001b[1;34m(history, smooth, smooth_factor)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msmoothed_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# Plot training and validation accuracy\n",
    "\n",
    "def plot_training_validation_acc(history, smooth=False, smooth_factor=0.8):\n",
    "    def smooth_curve(points, factor=0.8):\n",
    "        smoothed_points = []\n",
    "        for point in points:\n",
    "            if smoothed_points:\n",
    "                previous = smoothed_points[-1]\n",
    "                smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    if smooth:\n",
    "        acc = smooth_curve(acc)\n",
    "        val_acc = smooth_curve(val_acc)\n",
    "        loss = smooth_curve(loss)\n",
    "        val_loss = smooth_curve(val_loss)\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_training_validation_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accompanied-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                         Type                  Data/Info\n",
      "----------------------------------------------------------------\n",
      "Activation                       type                  <class 'tensorflow.python<...>.layers.core.Activation'>\n",
      "BS                               int                   20\n",
      "BatchNormalization               type                  <class 'tensorflow.python<...>n_v2.BatchNormalization'>\n",
      "Conv2D                           type                  <class 'tensorflow.python<...>rs.convolutional.Conv2D'>\n",
      "Dense                            type                  <class 'tensorflow.python<...>keras.layers.core.Dense'>\n",
      "Dropout                          type                  <class 'tensorflow.python<...>ras.layers.core.Dropout'>\n",
      "EPOCHS                           int                   30\n",
      "Flatten                          type                  <class 'tensorflow.python<...>ras.layers.core.Flatten'>\n",
      "INIT_LR                          float                 0.001\n",
      "ImageDataGenerator               type                  <class 'tensorflow.python<...>mage.ImageDataGenerator'>\n",
      "Input                            function              <function Input at 0x000001DF2E393820>\n",
      "Lambda                           type                  <class 'tensorflow.python<...>eras.layers.core.Lambda'>\n",
      "MaxPooling2D                     type                  <class 'tensorflow.python<...>rs.pooling.MaxPooling2D'>\n",
      "Model                            type                  <class 'tensorflow.python<...>s.engine.training.Model'>\n",
      "Sequential                       type                  <class 'tensorflow.python<...>e.sequential.Sequential'>\n",
      "VGG16                            function              <function VGG16 at 0x000001DF568BA160>\n",
      "base_model                       Functional            <tensorflow.python.keras.<...>ct at 0x000001DF58F34220>\n",
      "batch_size                       int                   20\n",
      "bottleneck_features_train        ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "bottleneck_features_validation   ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "createHeadLayers                 function              <function createHeadLayers at 0x000001DF5F84FE50>\n",
      "createMultiTaskModel             function              <function createMultiTask<...>el at 0x000001DEC17F5550>\n",
      "csv                              module                <module 'csv' from 'c:\\\\p<...>\\\\python38\\\\lib\\\\csv.py'>\n",
      "datagen                          ImageDataGenerator    <tensorflow.python.keras.<...>ct at 0x000001DF58D80E50>\n",
      "epochs                           int                   30\n",
      "generator                        DirectoryIterator     <tensorflow.python.keras.<...>ct at 0x000001DF58FD51C0>\n",
      "history                          History               <tensorflow.python.keras.<...>ct at 0x000001DF605737C0>\n",
      "image                            module                <module 'tensorflow.keras<...>ing\\\\image\\\\__init__.py'>\n",
      "img                              ndarray               1x150x150x3: 67500 elems, type `uint8`, 67500 bytes\n",
      "img_height                       int                   150\n",
      "img_to_array                     function              <function img_to_array at 0x000001DF56BD5670>\n",
      "img_width                        int                   150\n",
      "input_shape                      tuple                 n=4\n",
      "input_tensor                     KerasTensor           KerasTensor(type_spec=Ten<...>ated by layer 'input_1'\")\n",
      "inputs                           KerasTensor           KerasTensor(type_spec=Ten<...>ated by layer 'input_8'\")\n",
      "label_dataset                    function              <function label_dataset at 0x000001DF58D3C790>\n",
      "load_model                       function              <function load_model at 0x000001DF55AA3DC0>\n",
      "model                            Functional            <tensorflow.python.keras.<...>ct at 0x000001DF6050CE50>\n",
      "nb_train_samples                 int                   809\n",
      "nb_validation_samples            int                   809\n",
      "np                               module                <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "optimizers                       module                <module 'tensorflow.keras<...>optimizers\\\\__init__.py'>\n",
      "output                           KerasTensor           KerasTensor(type_spec=Ten<...>r 'type_classification'\")\n",
      "plot_training_validation_acc     function              <function plot_training_v<...>cc at 0x000001DF6023D310>\n",
      "plt                              module                <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "read_csv                         function              <function read_csv at 0x000001DF58D3CF70>\n",
      "tasks                            list                  n=1\n",
      "tf                               module                <module 'tensorflow' from<...>tensorflow\\\\__init__.py'>\n",
      "train_data                       ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "train_data_dir                   str                   images\n",
      "train_labels                     ndarray               809: 809 elems, type `int32`, 3236 bytes\n",
      "validation_data                  ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "validation_data_dir              str                   images\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
