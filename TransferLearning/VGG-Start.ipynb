{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aerial-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from ipynb.fs.full.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fuzzy-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building VGG16...\n",
      "Found 809 images belonging to 1 classes.\n",
      "Saving bottleneck features (train)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\PIL\\Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 809 images belonging to 1 classes.\n",
      "Saving bottleneck features (validation)...\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = 'images'\n",
    "validation_data_dir = 'images'\n",
    "nb_train_samples = 809\n",
    "nb_validation_samples = 809\n",
    "epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#test_dir = '../data/test'\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Save bottleneck features from VGG\n",
    "#\n",
    "\n",
    "# build the VGG16 network, leaving off the top classifier layer\n",
    "# so we just get the features as output\n",
    "print('Building VGG16...')\n",
    "input_tensor = Input(shape=(img_width, img_height, 3))\n",
    "base_model = VGG16(weights='imagenet', include_top=False,\n",
    "        input_tensor=input_tensor)\n",
    "\n",
    "# Save training features\n",
    "datagen = ImageDataGenerator(rescale=1/255)\n",
    "generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None, # class mode set to None here, because images are loaded in order\n",
    "    shuffle=False)\n",
    "\n",
    "print('Saving bottleneck features (train)...')\n",
    "bottleneck_features_train = base_model.predict(\n",
    "    generator, nb_train_samples // batch_size)\n",
    "np.save('features/train.npy', bottleneck_features_train)\n",
    "\n",
    "# Save validation features\n",
    "generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "print('Saving bottleneck features (validation)...')\n",
    "bottleneck_features_validation = base_model.predict(\n",
    "    generator, nb_validation_samples // batch_size)\n",
    "\n",
    "np.save('features/validation.npy',\n",
    "        bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thermal-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHeadLayers(inputs):\n",
    "    # setup params and where to save features\n",
    "    \n",
    "    # flatten the output convolutions, some implementations also \n",
    "    #. perform an average pooling here to collapse the features down\n",
    "    x = Flatten()(inputs)\n",
    "    # add two fully connected layers and some dropout\n",
    "    x = Dense(256)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid', name=\"type_classification\")(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "laden-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMultiTaskModel(inputShape, tasks):\n",
    "    \n",
    "    # construct both heads\n",
    "    inputs = Input(shape=inputShape)\n",
    "    output_grass = createHeadLayers(inputs)\n",
    "    \n",
    "    model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs= [output_grass],\n",
    "            name=\"pokemon\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "distinct-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 809 lines.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Epoch 1/30\n",
      "41/41 [==============================] - 1s 6ms/step - loss: 0.8905 - accuracy: 0.8842\n",
      "Epoch 2/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.3018 - accuracy: 0.9112\n",
      "Epoch 3/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.2675 - accuracy: 0.9142\n",
      "Epoch 4/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.2735 - accuracy: 0.9038\n",
      "Epoch 5/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1974 - accuracy: 0.9289\n",
      "Epoch 6/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1944 - accuracy: 0.9225\n",
      "Epoch 7/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1866 - accuracy: 0.9172\n",
      "Epoch 8/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.9255\n",
      "Epoch 9/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1651 - accuracy: 0.9331\n",
      "Epoch 10/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1252 - accuracy: 0.9505\n",
      "Epoch 11/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9431\n",
      "Epoch 12/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1083 - accuracy: 0.9507\n",
      "Epoch 13/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1227 - accuracy: 0.9426\n",
      "Epoch 14/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9528\n",
      "Epoch 15/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.9497\n",
      "Epoch 16/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0979 - accuracy: 0.9527\n",
      "Epoch 17/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9696\n",
      "Epoch 18/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9774\n",
      "Epoch 19/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9729\n",
      "Epoch 20/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9799\n",
      "Epoch 21/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0650 - accuracy: 0.9750\n",
      "Epoch 22/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 0.9848\n",
      "Epoch 23/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9825\n",
      "Epoch 24/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 0.9786\n",
      "Epoch 25/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0459 - accuracy: 0.9867\n",
      "Epoch 26/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0434 - accuracy: 0.9823\n",
      "Epoch 27/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.9830\n",
      "Epoch 28/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0573 - accuracy: 0.9703\n",
      "Epoch 29/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9820: 0s - loss: 0.0551 - accuracy: \n",
      "Epoch 30/30\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "INIT_LR = 1e-3\n",
    "BS = 20\n",
    "\n",
    "# Load bottleneck features training\n",
    "train_data = np.load('features/train.npy')\n",
    "\n",
    "# Load bottleneck features validation\n",
    "validation_data = np.load('features/validation.npy')\n",
    "\n",
    "tasks = ['Poison']\n",
    "\n",
    "train_labels = label_dataset(tasks[0], 'pokemon_2.csv')\n",
    "print(train_labels)\n",
    "inputs = Input(shape=input_shape[1:])\n",
    "output = createHeadLayers(inputs)\n",
    "\n",
    "data = load_images('images\\images', [150, 150])\n",
    "data = np.array(data, dtype='float')/255.0\n",
    "\n",
    "# Extracting labels\n",
    "ptypeLabels = label_dataset('Grass', 'pokemon_2.csv')\n",
    "\n",
    "# binarize labels\n",
    "\n",
    "\n",
    "split = train_test_split(data, ptypeLabels, test_size=0.2)\n",
    "(trainX, testX, trainTypeY, testTypeY) = split\n",
    "\n",
    "\n",
    "model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs= output,\n",
    "            name=\"pokemon\")\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_data, train_labels,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BS,\n",
    "              verbose=1)\n",
    "\n",
    "model.save('pokemon_model.h5', save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "insured-community",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=150x150 at 0x1DF94F086D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DF94F9A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img = image.load_img('images/images/venusaur.png', target_size = (150, 150))\n",
    "display(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "model = load_model('pokemon_model.h5')\n",
    "\n",
    "x = base_model.predict(img)\n",
    "result = model.predict(x)\n",
    "\n",
    "print(result)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "imported-laugh",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b2593a47bcca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mplot_training_validation_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-b2593a47bcca>\u001b[0m in \u001b[0;36mplot_training_validation_acc\u001b[1;34m(history, smooth, smooth_factor)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msmoothed_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# Plot training and validation accuracy\n",
    "\n",
    "def plot_training_validation_acc(history, smooth=False, smooth_factor=0.8):\n",
    "    def smooth_curve(points, factor=0.8):\n",
    "        smoothed_points = []\n",
    "        for point in points:\n",
    "            if smoothed_points:\n",
    "                previous = smoothed_points[-1]\n",
    "                smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    if smooth:\n",
    "        acc = smooth_curve(acc)\n",
    "        val_acc = smooth_curve(val_acc)\n",
    "        loss = smooth_curve(loss)\n",
    "        val_loss = smooth_curve(val_loss)\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_training_validation_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "subsequent-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                         Type                  Data/Info\n",
      "----------------------------------------------------------------\n",
      "Activation                       type                  <class 'tensorflow.python<...>.layers.core.Activation'>\n",
      "BS                               int                   20\n",
      "BatchNormalization               type                  <class 'tensorflow.python<...>n_v2.BatchNormalization'>\n",
      "Conv2D                           type                  <class 'tensorflow.python<...>rs.convolutional.Conv2D'>\n",
      "Dense                            type                  <class 'tensorflow.python<...>keras.layers.core.Dense'>\n",
      "Dropout                          type                  <class 'tensorflow.python<...>ras.layers.core.Dropout'>\n",
      "EPOCHS                           int                   30\n",
      "Flatten                          type                  <class 'tensorflow.python<...>ras.layers.core.Flatten'>\n",
      "INIT_LR                          float                 0.001\n",
      "ImageDataGenerator               type                  <class 'tensorflow.python<...>mage.ImageDataGenerator'>\n",
      "Input                            function              <function Input at 0x000001DF2E393820>\n",
      "Lambda                           type                  <class 'tensorflow.python<...>eras.layers.core.Lambda'>\n",
      "MaxPooling2D                     type                  <class 'tensorflow.python<...>rs.pooling.MaxPooling2D'>\n",
      "Model                            type                  <class 'tensorflow.python<...>s.engine.training.Model'>\n",
      "Sequential                       type                  <class 'tensorflow.python<...>e.sequential.Sequential'>\n",
      "VGG16                            function              <function VGG16 at 0x000001DF568BA160>\n",
      "base_model                       Functional            <tensorflow.python.keras.<...>ct at 0x000001DF58F34220>\n",
      "batch_size                       int                   20\n",
      "bottleneck_features_train        ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "bottleneck_features_validation   ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "createHeadLayers                 function              <function createHeadLayers at 0x000001DF5F84FE50>\n",
      "createMultiTaskModel             function              <function createMultiTask<...>el at 0x000001DEC17F5550>\n",
      "csv                              module                <module 'csv' from 'c:\\\\p<...>\\\\python38\\\\lib\\\\csv.py'>\n",
      "datagen                          ImageDataGenerator    <tensorflow.python.keras.<...>ct at 0x000001DF58D80E50>\n",
      "epochs                           int                   30\n",
      "generator                        DirectoryIterator     <tensorflow.python.keras.<...>ct at 0x000001DF58FD51C0>\n",
      "history                          History               <tensorflow.python.keras.<...>ct at 0x000001DF605737C0>\n",
      "image                            module                <module 'tensorflow.keras<...>ing\\\\image\\\\__init__.py'>\n",
      "img                              ndarray               1x150x150x3: 67500 elems, type `uint8`, 67500 bytes\n",
      "img_height                       int                   150\n",
      "img_to_array                     function              <function img_to_array at 0x000001DF56BD5670>\n",
      "img_width                        int                   150\n",
      "input_shape                      tuple                 n=4\n",
      "input_tensor                     KerasTensor           KerasTensor(type_spec=Ten<...>ated by layer 'input_1'\")\n",
      "inputs                           KerasTensor           KerasTensor(type_spec=Ten<...>ated by layer 'input_8'\")\n",
      "label_dataset                    function              <function label_dataset at 0x000001DF58D3C790>\n",
      "load_model                       function              <function load_model at 0x000001DF55AA3DC0>\n",
      "model                            Functional            <tensorflow.python.keras.<...>ct at 0x000001DF6050CE50>\n",
      "nb_train_samples                 int                   809\n",
      "nb_validation_samples            int                   809\n",
      "np                               module                <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "optimizers                       module                <module 'tensorflow.keras<...>optimizers\\\\__init__.py'>\n",
      "output                           KerasTensor           KerasTensor(type_spec=Ten<...>r 'type_classification'\")\n",
      "plot_training_validation_acc     function              <function plot_training_v<...>cc at 0x000001DF6023D310>\n",
      "plt                              module                <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "read_csv                         function              <function read_csv at 0x000001DF58D3CF70>\n",
      "tasks                            list                  n=1\n",
      "tf                               module                <module 'tensorflow' from<...>tensorflow\\\\__init__.py'>\n",
      "train_data                       ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "train_data_dir                   str                   images\n",
      "train_labels                     ndarray               809: 809 elems, type `int32`, 3236 bytes\n",
      "validation_data                  ndarray               809x4x4x512: 6627328 elems, type `float32`, 26509312 bytes (25.28125 Mb)\n",
      "validation_data_dir              str                   images\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
